{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d2fe7a1",
   "metadata": {},
   "source": [
    "# Automated Design of Agentic Systems (ADAS)\n",
    "\n",
    "Automated Design of Agentic Systems (ADAS) is a [paper by Hu et al.](https://arxiv.org/pdf/2408.08435) which was one of the first works to explore the idea of automatically designing agentic systems:\n",
    "\n",
    ">Automated Design of Agentic Systems (ADAS) [...] aims to automatically create powerful agentic system designs, including inventing novel building blocks\n",
    "and/or combining them in new ways.\n",
    "\n",
    "The idea of ADAS is that instead of using hand-crafted prompting techniques like LLM debate, a meta agent designs novel agents using code. This agent is then tested on a task and the results are given back to the meta agent to further guide its search for a design.\n",
    "\n",
    "![](assets/adas.png)\n",
    "\n",
    "```\n",
    "@article{hu2024ADAS,\n",
    "title={Automated Design of Agentic Systems},\n",
    "author={Hu, Shengran and Lu, Cong and Clune, Jeff},\n",
    "journal={arXiv preprint arXiv:2408.08435},\n",
    "year={2024}\n",
    "}\n",
    "```\n",
    "\n",
    "This notebook shows how to implement their ideas using the agenticblocks framework."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b566d4",
   "metadata": {},
   "source": [
    "To implement ADAS we need to build the following:\n",
    "\n",
    "1. The meta agent which has the task to design agentic systems. We give it access to the built-in blocks of agenticblocks and ask it to come up with novel blocks and / or combine them in new ways.\n",
    "2. A way of testing the designs found by the meta agent, so it can use the feedback as input for new designs. For demonstration purposes we will use a small sample of the [MMLU-STEM Dataset](https://huggingface.co/datasets/TIGER-Lab/MMLU-STEM), containing multiple choice questions from academic exams and textbooks in the STEM domain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ce00a6",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f9121d",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "Besides some built-in python libraries, we import the agenticblocks library to define the agentic system, pyarrow to read the MMLU dataset, numpy and matplotlib to compute and plot metrics and tqdm to show progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab848f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/robin/projects/agenticblocks/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "import io\n",
    "import json\n",
    "import urllib.request\n",
    "import traceback\n",
    "\n",
    "import agenticblocks as ab\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pyarrow.parquet as pq\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3048b680",
   "metadata": {},
   "source": [
    "This example involves running model-generated code. If you set `YOLO = False` you will be prompted to review and accept any model-generated code before it is executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60770bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "YOLO = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e4ff53",
   "metadata": {},
   "source": [
    "### Model Access\n",
    "\n",
    "We need to set up access to the language model(s) we want to use.\n",
    "agenticblocks supports all OpenAI API compatible providers.\n",
    "\n",
    "You can set the base url and api key via the `OPENAI_API_URL` and `OPENAI_API_KEY` environment variables.\n",
    "\n",
    "For more details check the [getting started example](01_getting_started.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38bca8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import dotenv\n",
    "#dotenv.load_dotenv()\n",
    "\n",
    "#!export OPENAI_API_URL=\n",
    "#!export OPENAI_API_KEY="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52178805",
   "metadata": {},
   "source": [
    "We also define the models we are going to use throughout this notebook.\n",
    "For this example we will use gpt-4o for the meta agent and gpt-3.5-turbo as the agent in the discovered blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d68f528",
   "metadata": {},
   "outputs": [],
   "source": [
    "META_MODEL_NAME = \"openai/gpt-4o\"\n",
    "BLOCK_MODEL_NAME = \"openai/gpt-3.5-turbo\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622e9ea0",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "Let's download the MMLU STEM dataset from Hugging Face.\n",
    "\n",
    "We will use a small subset of 50 examples for providing feedback to the meta agent on how well the discovered block perfomed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "553c16a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 50\n",
    "url = \"https://huggingface.co/datasets/TIGER-Lab/MMLU-STEM/resolve/main/data/test-00000-of-00001.parquet\"\n",
    "\n",
    "with urllib.request.urlopen(url) as response:\n",
    "    data = io.BytesIO(response.read())\n",
    "\n",
    "df = pq.read_table(data).to_pandas().sample(n_samples, random_state=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0c66bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_input(row):\n",
    "    return f\"\"\"{row['question']}\n",
    "\n",
    "{chr(10).join([f\"{chr(ord('A')+i)}) {choice}\" for i, choice in enumerate(row['choices'])])}\n",
    "\"\"\"\n",
    "\n",
    "df[\"input\"] = df.apply(format_input, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d3f02a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statement 1 | For any two groups G and G', there exists a homomorphism of G into G'. Statement 2 | Every homomorphism is a one-to-one map.\n",
      "\n",
      "A) True, True\n",
      "B) False, False\n",
      "C) True, False\n",
      "D) False, True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df[\"input\"].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a0a122",
   "metadata": {},
   "source": [
    "## The Meta Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42aa3a28",
   "metadata": {},
   "source": [
    "The meta agent itself is just an instance of the agenticblocks `Model` class. The same class the agent will be use to create its models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0934fa69",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_agent = ab.Model(\n",
    "    META_MODEL_NAME,\n",
    "    keep_history=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16ec684",
   "metadata": {},
   "source": [
    "We also need to define the prompt for the meta agent. In the prompt we let the meta agent know ...\n",
    "* about its task - building an agentic system to answer MMLU questions\n",
    "* what tools it can use to build these systems - python code and the agenticblocks framework\n",
    "* how its blocks will be evaluated on the dataset\n",
    "* how its previous solutions performed by using an \"archive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57edfc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks = [ab.IO, ab.ChainOfThought, ab.SelfConsistency, ab.MultiAgentDebate, ab.SelfRefine]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce5faf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"# Overview\n",
    "You are an expert machine learning researcher testing various agentic systems. Your objective is to design building blocks such as prompts and control flows within these systems to solve complex tasks. Your aim is to design an optimal agent performing well on the MMLU (Massive Multitask Language Understanding) benchmark, a challenging evaluation that assesses a model's ability to answer questions across a wide range of subjects and difficulty levels. It includes subjects from STEM, social sciences, humanities, and more.\n",
    "\n",
    "## An example question from MMLU:\n",
    "The constellation ... is a bright W-shaped constellation in the northern sky.\n",
    "\n",
    "A) Centaurus\n",
    "B) Cygnus\n",
    "C) Cassiopeia\n",
    "D) Cepheus\n",
    "\n",
    "# How to implement your agentic systems\n",
    "You can use the agenticblocks library to create and manage models:\n",
    "\n",
    "```python\n",
    "import agenticblocks as ab\n",
    "model = ab.Model(\"{BLOCK_MODEL_NAME}\", system_prompt=\"You are a helpful assistant.\")\n",
    "```\n",
    "\n",
    "Always use the {BLOCK_MODEL_NAME} model when you create a new model.\n",
    "A model can be prompted in the following way:\n",
    "\n",
    "```python\n",
    "model(\"Your prompt here\", temperature=0.7)\n",
    "```\n",
    "\n",
    "blocks define how models are prompted and how they interact with each other.\n",
    "agenticblocks offers some built-in blocks that you can use to create agentic systems:\n",
    "\n",
    "```python\n",
    "{'\\n\\n\\n'.join([inspect.getsource(block) for block in blocks])}\n",
    "```\n",
    "\n",
    "The built-in blocks can be imported using `from agenticblocks import BlockName`.\n",
    "You can use these blocks as building blocks to combine them to new blocks or use them as inspiration to design your own blocks.\n",
    "\n",
    "# Discovered architecture archive\n",
    "Here is the archive of the built-in blocks and discovered architectures:\n",
    "\n",
    "[ARCHIVE]\n",
    "\n",
    "The fitness value is the median and 95% Bootstrap Confidence Interval of the correct rate on a validation question set. Your GOAL is to maximize the \"fitness\".\n",
    "\n",
    "# Output Instruction and Example:\n",
    "You need to output a JSON object with three keys: \"thought\", \"name\", and \"code\".\n",
    "The first key should be (\"thought\"), and it should capture your thought process for designing the next function. In the \"thought\" section, first reason about what should be the next interesting agent to try, then describe your reasoning and the overall concept behind the agent design, and finally detail the implementation steps.\n",
    "The second key (\"name\") corresponds to the class name of your next agent architecture. Make sure the name is unique and descriptive of the architecture you are proposing. \n",
    "Finally, the last key (\"code\") corresponds to the exact Python code of the class that you would like to try. You must write a COMPLETE CODE in \"code\": Your code will be part of the entire project, so please implement complete, reliable, reusable code snippets.\n",
    "\n",
    "Here is an example of the output format for the next agent architecture:\n",
    "\n",
    "{{\n",
    "    \"thought\": \"**Insights:**\\nYour insights on what should be the next interesting agent.\\n**Overall Idea:**\\nyour reasoning and the overall concept behind the agent design.\\n**Implementation:**\\ndescribe the implementation step by step.\",\n",
    "    \"name\": \"YourCustomAgenticBlock\",\n",
    "    \"code\": '''class YourCustomAgenticBlock:\n",
    "    def __init__(self):\n",
    "        # Your code here\n",
    "\n",
    "    def __call__(self, question):\n",
    "        # Your code here\n",
    "'''\n",
    "}}\n",
    "\n",
    "You must use the exact interface used above. You need to specify the instruction, input information, and the required output fields for various LLM agents to do their specific part of the architecture. \n",
    "Also, it could be helpful to set the LLM’s system_prompt and temperature to further control the LLM’s response. Note that only the question will be passed to the instance of the block class. Everything else needs to be already implemented in the __init__ and __call__ methods of the block class.\n",
    "DO NOT FORGET to pass the task description to models inside the block class if you think it is needed, otherwise the models will not know about the task.\n",
    "\n",
    "The key \"code\" from your output JSON will be saved to a custom_blocks.py file and will be called like this:\n",
    "\n",
    "```python\n",
    "from custom_blocks import YourCustomAgenticBlock\n",
    "\n",
    "question = '''The constellation ... is a bright W-shaped constellation in the northern sky.\n",
    "\n",
    "A) Centaurus\n",
    "B) Cygnus\n",
    "C) Cassiopeia\n",
    "D) Cepheus\n",
    "'''\n",
    "\n",
    "block = YourCustomAgenticBlock()\n",
    "result = block(question)\n",
    "```\n",
    "\n",
    "# Your task\n",
    "You are deeply familiar with LLM prompting techniques and LLM agent works from the literature. Your goal is to maximize \"fitness\" by proposing interestingly new agents. \n",
    "Observe the discovered architectures carefully and think about what insights, lessons, or stepping stones can be learned from them.\n",
    "Be creative to think about the next interesting architecture to try. You are encouraged to draw inspiration from related LLM agent papers or academic papers from other research areas.\n",
    "Using the knowledge learned from the archive and the inspiration from academic literature to give the next interesting architecture.\n",
    "THINK OUTSIDE THE BOX.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a5eed8",
   "metadata": {},
   "source": [
    "We will prompt the meta agent in a self refining manner. That means we ask it one time to reflect on its design and improve it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed44a9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "reflection_template = \"\"\"\"{previous_response}Carefully review the proposed new architecture and reflect on the following points:\"\n",
    "\n",
    "1. **Interestingness**: Assess whether your proposed architecture is interesting or innovative compared to existing methods in the archive. If you determine that the proposed architecture is not interesting, suggest a new architecture that addresses these shortcomings. \n",
    "- Make sure to check the difference between the proposed architecture and previous attempts.\n",
    "- Compare the proposal and the architectures in the archive CAREFULLY, including their actual differences in the implementation.\n",
    "- Decide whether the current architecture is innovative.\n",
    "- USE CRITICAL THINKING!\n",
    "\n",
    "2. **Implementation Mistakes**: Identify any mistakes you may have made in the implementation. Review the code carefully, debug any issues you find, and provide a corrected version. REMEMBER checking \"## WRONG Implementation examples\" in the prompt.\n",
    "\n",
    "3. **Improvement**: Based on the proposed architecture, suggest improvements in the detailed implementation that could increase its performance or effectiveness. In this step, focus on refining and optimizing the existing implementation without altering the overall design framework, except if you want to propose a different architecture if the current is not interesting.\n",
    "- Observe carefully about whether the implementation is actually doing what it is supposed to do.\n",
    "- Check if there is redundant code or unnecessary steps in the implementation. Replace them with effective implementation.\n",
    "- Try to avoid the implementation being too similar to the previous agent.\n",
    "\n",
    "And then, you need to improve or revise the implementation, or implement the new proposed architecture based on the reflection.\n",
    "\n",
    "Your response should be organized as follows:\n",
    "\n",
    "\"reflection\": Provide your thoughts on the interestingness of the architecture, identify any mistakes in the implementation, and suggest improvements.\n",
    "\n",
    "\"thought\": Revise your previous proposal or propose a new architecture if necessary, using the same format as the example response.\n",
    "\n",
    "\"name\": Provide a name for the revised or new architecture. (Don't put words like \"new\" or \"improved\" in the name.)\n",
    "\n",
    "\"code\": Provide the corrected code or an improved implementation. Make sure you actually implement your fix and improvement in this code.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd6141e",
   "metadata": {},
   "source": [
    "This is an adjusted prompt from the [ADAS repo](https://github.com/ShengranHu/ADAS).\n",
    "\n",
    "For simplicity, we tell the meta agent to always use the `MODEL_NAME` defined above for its blocks.\n",
    "If you have more models available, you can modify the prompt to let the meta agent decide which models it wants to use.\n",
    "\n",
    "We also show the meta agent the built in blocks and how it can design its own blocks.\n",
    "\n",
    "The archive is initialized by evaluating the built in blocks on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb5c1553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to evaluate a block on the mmlu dataset\n",
    "def eval_block(block, df):\n",
    "    answers = []\n",
    "    for question in df[\"input\"]:\n",
    "        response = block(f\"{question}\\n\\nThe last line of your answer should be the correct choice, e.g. A)\")[\"content\"]\n",
    "        answer = None\n",
    "        for line in response.splitlines():\n",
    "            if answer is not None:\n",
    "                break\n",
    "            for k, v in {\"A)\": 0, \"B)\": 1, \"C)\": 2, \"D)\": 3}.items():\n",
    "                if k in line:\n",
    "                    answer = v\n",
    "                    break\n",
    "        answers += [answer]\n",
    "    df[str(block)] = answers\n",
    "    # bootstrap score\n",
    "    acc_list = (df[\"answer\"] == df[str(block)]).astype(int)\n",
    "    scores = []\n",
    "    for _ in range(1000):\n",
    "        scores += [np.mean(np.random.choice(acc_list, len(acc_list), replace=True))]\n",
    "    return np.percentile(scores, 2.5), np.percentile(scores, 97.5), np.median(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fad9bb12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [16:02<00:00, 192.46s/it]\n"
     ]
    }
   ],
   "source": [
    "# build the initial archive by evaluating the built-in blocks on the train set\n",
    "archive = []\n",
    "for block_class in tqdm(blocks, total=len(blocks)):\n",
    "    model = ab.Model(BLOCK_MODEL_NAME)\n",
    "    if block_class == ab.MultiAgentDebate:\n",
    "        block = block_class(agents=[model]*3)\n",
    "    else:\n",
    "        block = block_class(model)\n",
    "    ci_lo, ci_hi, med = eval_block(block, df)\n",
    "    archive += [{\n",
    "        \"name\": f\"ab.{block_class.__name__}\",\n",
    "        \"thought\": \"agenticblocks built-in\",\n",
    "        \"code\": inspect.getsource(block_class),\n",
    "        \"fitness\": {\"95% Bootstrap Confidence Interval\": f\"({ci_lo*100:.1f}%, {ci_hi*100:.1f}%)\", \"median\": round(med*100, 1)},\n",
    "    }]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb0a44f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'ab.IO',\n",
       "  'thought': 'agenticblocks built-in',\n",
       "  'code': 'class IO:\\n    \"\"\"IO block - simple pass-through to the model.\"\"\"\\n\\n    def __init__(self, model):\\n        self.model = model\\n\\n    def __repr__(self):\\n        return f\"IO({self.model!r})\"\\n\\n    def __call__(self, prompt: str, **kwargs) -> dict:\\n        return self.model(prompt, **kwargs)\\n',\n",
       "  'fitness': {'95% Bootstrap Confidence Interval': '(36.0%, 64.0%)',\n",
       "   'median': np.float64(50.0)}},\n",
       " {'name': 'ab.ChainOfThought',\n",
       "  'thought': 'agenticblocks built-in',\n",
       "  'code': 'class ChainOfThought:\\n    \"\"\"Chain of Thought block - prompts the model to think step by step.\"\"\"\\n\\n    def __init__(self, model, template: str = \"{prompt}\\\\nLet\\'s think step by step.\"):\\n        self.model = model\\n        self.template = template\\n\\n    def __repr__(self):\\n        return f\"ChainOfThought({self.model!r})\"\\n\\n    def __call__(self, prompt: str, **kwargs) -> dict:\\n        return self.model(self.template.format(prompt=prompt), **kwargs)\\n',\n",
       "  'fitness': {'95% Bootstrap Confidence Interval': '(40.0%, 68.0%)',\n",
       "   'median': np.float64(54.0)}},\n",
       " {'name': 'ab.SelfConsistency',\n",
       "  'thought': 'agenticblocks built-in',\n",
       "  'code': 'class SelfConsistency:\\n    \"\"\"Self-Consistency block - runs a block N times and aggregates responses.\"\"\"\\n\\n    def __init__(\\n        self,\\n        block,\\n        n: int = 5,\\n        temperature: float = 0.7,\\n        aggregator=None,\\n        aggregator_template: str = \"{responses}\\\\nGiven the responses above. Output the most common answer.\",\\n    ):\\n        self.block = block\\n        self.n = n\\n        self.temperature = temperature\\n        self.aggregator = aggregator\\n        self.aggregator_template = aggregator_template\\n\\n    def __repr__(self):\\n        return f\"SelfConsistency({self.block!r}, n={self.n})\"\\n\\n    def __call__(self, prompt: str, **kwargs) -> dict:\\n        responses = []\\n        for _ in range(self.n):\\n            result = self.block(prompt, temperature=self.temperature, **kwargs)\\n            responses.append(result[\"content\"])\\n\\n        responses_text = \"\\\\n\\\\n\".join(responses)\\n\\n        if self.aggregator is None:\\n            return {\"content\": responses_text, \"extra\": {\"responses\": responses}}\\n\\n        return self.aggregator(self.aggregator_template.format(responses=responses_text))\\n',\n",
       "  'fitness': {'95% Bootstrap Confidence Interval': '(48.0%, 74.0%)',\n",
       "   'median': np.float64(62.0)}},\n",
       " {'name': 'ab.MultiAgentDebate',\n",
       "  'thought': 'agenticblocks built-in',\n",
       "  'code': 'class MultiAgentDebate:\\n    \"\"\"Multi-Agent Debate block - multiple agents debate to reach a consensus.\"\"\"\\n\\n    def __init__(\\n        self,\\n        agents: list,\\n        rounds: int = 2,\\n        moderator=None,\\n        debate_template: str = \"Question: {prompt}\\\\n\\\\nPrevious responses:\\\\n{history}\\\\n\\\\nProvide your response, considering the perspectives above:\",\\n        final_template: str = \"Question: {prompt}\\\\n\\\\nDebate summary:\\\\n{debate_history}\\\\n\\\\nBased on this debate, provide the final answer:\",\\n    ):\\n        self.agents = agents\\n        self.rounds = rounds\\n        self.moderator = moderator\\n        self.debate_template = debate_template\\n        self.final_template = final_template\\n\\n    def __repr__(self):\\n        return f\"MultiAgentDebate({self.agents!r}, rounds={self.rounds})\"\\n\\n    def __call__(self, prompt: str, **kwargs) -> dict:\\n        debate_history = []\\n\\n        # Initial round - each agent responds to the prompt\\n        for i, agent in enumerate(self.agents):\\n            result = agent(prompt, **kwargs)\\n            debate_history.append(f\"Agent {i + 1}: {result[\\'content\\']}\")\\n\\n        # Debate rounds\\n        for _ in range(self.rounds):\\n            history_text = \"\\\\n\\\\n\".join(debate_history)\\n            round_responses = []\\n\\n            for i, agent in enumerate(self.agents):\\n                result = agent(\\n                    self.debate_template.format(prompt=prompt, history=history_text),\\n                    **kwargs,\\n                )\\n                round_responses.append(f\"Agent {i + 1}: {result[\\'content\\']}\")\\n\\n            debate_history.extend(round_responses)\\n\\n        # Final synthesis\\n        full_history = \"\\\\n\\\\n\".join(debate_history)\\n\\n        if self.moderator is not None:\\n            final_result = self.moderator(\\n                self.final_template.format(prompt=prompt, debate_history=full_history),\\n                **kwargs,\\n            )\\n        else:\\n            # Use last agent as moderator if none provided\\n            final_result = self.agents[-1](\\n                self.final_template.format(prompt=prompt, debate_history=full_history),\\n                **kwargs,\\n            )\\n\\n        return {\\n            \"content\": final_result[\"content\"],\\n            \"extra\": {\"debate_history\": debate_history},\\n        }\\n',\n",
       "  'fitness': {'95% Bootstrap Confidence Interval': '(42.0%, 70.0%)',\n",
       "   'median': np.float64(56.0)}},\n",
       " {'name': 'ab.SelfRefine',\n",
       "  'thought': 'agenticblocks built-in',\n",
       "  'code': 'class SelfRefine:\\n    \"\"\"Self-Refine block - iteratively critiques and improves responses.\"\"\"\\n\\n    def __init__(\\n        self,\\n        model,\\n        iterations: int = 2,\\n        critique_template: str = \"Task: {prompt}\\\\n\\\\nResponse:\\\\n{response}\\\\n\\\\nCritique this response. What are its weaknesses? How can it be improved?\",\\n        refine_template: str = \"Task: {prompt}\\\\n\\\\nPrevious response:\\\\n{response}\\\\n\\\\nCritique:\\\\n{critique}\\\\n\\\\nProvide an improved response addressing the critique:\",\\n    ):\\n        self.model = model\\n        self.iterations = iterations\\n        self.critique_template = critique_template\\n        self.refine_template = refine_template\\n\\n    def __repr__(self):\\n        return f\"SelfRefine({self.model!r}, iterations={self.iterations})\"\\n\\n    def __call__(self, prompt: str, **kwargs) -> dict:\\n        # Generate initial response\\n        result = self.model(prompt, **kwargs)\\n        response = result[\"content\"]\\n\\n        history = [{\"response\": response, \"critique\": None}]\\n\\n        # Iterative refinement\\n        for _ in range(self.iterations):\\n            # Critique\\n            critique_result = self.model(\\n                self.critique_template.format(prompt=prompt, response=response),\\n                **kwargs,\\n            )\\n            critique = critique_result[\"content\"]\\n\\n            # Refine\\n            refine_result = self.model(\\n                self.refine_template.format(prompt=prompt, response=response, critique=critique),\\n                **kwargs,\\n            )\\n            response = refine_result[\"content\"]\\n\\n            history.append({\"response\": response, \"critique\": critique})\\n\\n        return {\\n            \"content\": response,\\n            \"extra\": {\"history\": history},\\n        }\\n',\n",
       "  'fitness': {'95% Bootstrap Confidence Interval': '(30.0%, 58.0%)',\n",
       "   'median': np.float64(44.0)}}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "archive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06c8d92",
   "metadata": {},
   "source": [
    "Now let's use the prompts to let the meta agent search for new blocks, evaluate them on the mmlu dataset and append new blocks to the archive.\n",
    "\n",
    "For demonstration purposes we do this for 15 rounds. For better results add more rounds - in the ADAS paper 25 iterations are used.\n",
    "\n",
    "If the code produced by the meta agent throws an exception we ask it up to 5 times to fix it.\n",
    "\n",
    "If you notice the meta agent keeps repeating a certain mistake or uses a bad pattern in the generated blocks you can adjust the prompt above and ask it to avoid these mistakes.\n",
    "\n",
    "With `YOLO = False`, you need to confirm each new block before it is evaluated on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "46961744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================== Round 1 of 15 ==============================\n",
      "Error (attempt 1/3): Expecting ',' delimiter: line 5 column 15 (char 2489)\n",
      "Successfully found a new block:\n",
      "{\n",
      "    \"name\": \"RefinedAdaptiveDynamicStrategy\",\n",
      "    \"thought\": \"**Insights:**\\nThe dynamic agent design leveraging adaptability in selecting strategies based on question complexity is compelling. Its hypothesis is rooted in adapting to varying levels of question difficulty using different models, improving response accuracy and efficiency. However, it's clear from the reflected issue that detailed attention on JSON structure for responses is vital.\\n\\n**Overall Idea:**\\nAddress the JSON encoding error found in the previous implementation while maintaining the design principle that involves assessing the question's complexity dynamically, caching results, and adjusting methodology accordingly to deliver optimal responses based on complexity and confidence levels.\\n\\n**Implementation:**\\nTo prevent such issues, the implementation involves refining the dynamic block to function correctly without JSON formatting errors while retaining its flexibility and adaptive behavior. It systematically proceeds through complexity assessment, employs a caching strategy, and optimizes response selection process smartly.\",\n",
      "    \"code\": \"import agenticblocks as ab\\nfrom collections import defaultdict\\n\\nclass RefinedAdaptiveDynamicStrategy:\\n    def __init__(self):\\n        self.model = ab.Model(\\\"openai/gpt-3.5-turbo\\\", system_prompt=\\\"You are a helpful assistant.\\\")\\n        self.io_block = ab.IO(self.model)\\n        self.chain_of_thought_block = ab.ChainOfThought(self.model)\\n        self.self_consistency_block = ab.SelfConsistency(self.chain_of_thought_block, n=5, temperature=0.7)\\n        self.complexity_cache = defaultdict(lambda: None)\\n\\n    def _assess_complexity(self, prompt, **kwargs):\\n        if prompt in self.complexity_cache:\\n            return self.complexity_cache[prompt]\\n\\n        assessment_prompt = f\\\"Task: Assess the difficulty level of the following question and provide reasons.\\\\n\\\\nQuestion: {prompt}\\\\n\\\\nRate the complexity from 1 (simple) to 3 (complex), include a confidence score.\\\"\\n        result = self.model(assessment_prompt, **kwargs)\\n        complexity_rating = result[\\\"content\\\"].strip().lower()\\n\\n        self.complexity_cache[prompt] = complexity_rating\\n        return complexity_rating\\n\\n    def __call__(self, prompt: str, **kwargs) -> dict:\\n        complexity_response = self._assess_complexity(prompt, temperature=0.5)\\n\\n        if complexity_response:\\n            if \\\"1\\\" in complexity_response:\\n                return self.io_block(prompt, **kwargs)\\n            elif \\\"2\\\" in complexity_response:\\n                return self.chain_of_thought_block(prompt, **kwargs)\\n            else:\\n                return self.self_consistency_block(prompt, **kwargs)\\n        else:\\n            return self.io_block(prompt, **kwargs)  # Default to IO for unknown classification\\n\",\n",
      "    \"fitness\": {\n",
      "        \"95% Bootstrap Confidence Interval\": \"(50.0%, 76.0%)\",\n",
      "        \"median\": 62.0\n",
      "    }\n",
      "}\n",
      "============================== Round 2 of 15 ==============================\n",
      "Successfully found a new block:\n",
      "{\n",
      "    \"name\": \"AdvancedExpertPanel\",\n",
      "    \"thought\": \"**Insights:**\\nTaking into account the existing innovation in the 'ExpertPanelArchitecture', incorporating multiple strategies within each expert agent to account for complexity and refinement could enhance performance. Specialized strategies can improve precision in domain-specific contexts.\\n\\n**Overall Idea:**\\nEnhance the 'ExpertPanelArchitecture' to utilize multiple specialized agents per subject domain. Each agent employs diverse blocks like SelfRefine and MultiAgentDebate tailored to each subject's nature, allowing for iterative refinement and consensus-building.\\n\\n**Implementation:**\\n1. Use specialized blocks like SelfRefine and MultiAgentDebate within each subject domain.\\n2. Incorporate a feedback loop that assesses correctness based on synthetic validation questions per domain.\\n3. Implement different strategies for different complexity within each domain, ensuring robust handling of simple to complex queries.\\n4. Refine models and caching responses progressively to accumulate successful patterns and strategies over time.\",\n",
      "    \"code\": \"import agenticblocks as ab\\n\\nclass AdvancedExpertPanel:\\n    def __init__(self):\\n        self.model = ab.Model(\\\"openai/gpt-3.5-turbo\\\", system_prompt=\\\"You are a specialized expert assistant.\\\")\\n        self.subjects = ['STEM', 'Humanities', 'Social Sciences']\\n        self.expert_agents = {\\n            'STEM': [\\n                ab.SelfRefine(self.model, iterations=3),\\n                ab.MultiAgentDebate([ab.ChainOfThought(self.model) for _ in range(3)], rounds=2)\\n            ],\\n            'Humanities': [\\n                ab.SelfRefine(self.model, iterations=2),\\n                ab.ChainOfThought(self.model)\\n            ],\\n            'Social Sciences': [\\n                ab.MultiAgentDebate([ab.ChainOfThought(self.model) for _ in range(5)], rounds=3)\\n            ]\\n        }\\n        self.io_block = ab.IO(self.model)\\n\\n    def _classify_subject(self, prompt, **kwargs):\\n        classification_prompt = f\\\"Task: Classify the following question into a subject domain from: STEM, Humanities, Social Sciences.\\\\n\\\\nQuestion: {prompt}\\\"\\n        result = self.model(classification_prompt, temperature=0, **kwargs)\\n        return result['content'].strip()\\n\\n    def _synthesize_responses(self, responses):\\n        # Simple heuristic to synthesize responses\\n        return responses[0]\\n\\n    def __call__(self, prompt: str, **kwargs) -> dict:\\n        subject_domain = self._classify_subject(prompt)\\n        if subject_domain in self.expert_agents:\\n            responses = []\\n            for agent in self.expert_agents[subject_domain]:\\n                responses.append(agent(prompt, **kwargs)['content'])\\n            response = self._synthesize_responses(responses)\\n        else:\\n            response = self.io_block(prompt, **kwargs)['content']\\n\\n        return {\\n            \\\"content\\\": response,\\n            \\\"extra\\\": {\\\"subject_domain\\\": subject_domain}\\n        }\",\n",
      "    \"fitness\": {\n",
      "        \"95% Bootstrap Confidence Interval\": \"(44.0%, 72.0%)\",\n",
      "        \"median\": 58.0\n",
      "    }\n",
      "}\n",
      "============================== Round 3 of 15 ==============================\n",
      "Successfully found a new block:\n",
      "{\n",
      "    \"name\": \"DynamicStrategicBlender\",\n",
      "    \"thought\": \"**Insights:**\\nBuilding on the idea of dynamically adapting strategies based on question context, it's vital to consider multi-factor determination rather than binary classification. Enhancing adaptability can be achieved through a 'DynamicStrategicBlender' that assesses and chooses a combination of strategies based on detailed feature extraction from the question.\\n\\n**Overall Idea:**\\n1. Utilize a multi-label classifier to categorize question requirements as intuition, algorithmic, or mixed strategy.\\n2. Employ hybrid strategies when necessary, allowing a seamless blend of ChainOfThought, SelfConsistency, and MultiAgentDebate for comprehensive assessments.\\n3. Provide feedback loops to enhance decision accuracy over time.\\n\\n**Implementation:**\\n1. Replace binary classification with multi-label assessment including 'intuition', 'algorithm', or 'mixed'.\\n2. Integrate hybrid strategy execution using the assigned techniques.\\n3. Include feedback loop to refine the strategy selection in subsequent iterations.\",\n",
      "    \"code\": \"import agenticblocks as ab\\n\\nclass DynamicStrategicBlender:\\n    def __init__(self):\\n        self.model = ab.Model(\\\"openai/gpt-3.5-turbo\\\", system_prompt=\\\"You are an adaptive strategic assistant.\\\")\\n        self.intuition_block = ab.ChainOfThought(self.model)\\n        self.algorithm_block = ab.SelfConsistency(ab.IO(self.model), n=5, temperature=0.7)\\n        self.debate_block = ab.MultiAgentDebate([ab.ChainOfThought(self.model) for _ in range(3)], rounds=2)\\n\\n    def _assess_context(self, prompt, **kwargs):\\n        context_prompt = (\\n            f\\\"Task: Determine whether the following question requires 'intuition', 'algorithm', or a 'mixed' strategy.\\\\n\\\"\\n            f\\\"Question: {prompt}\\\\nExplain your reasoning with examples of similar question types.\\\"\\n        )\\n        result = self.model(context_prompt, temperature=0, **kwargs)\\n        return result['content'].strip().lower()\\n\\n    def __call__(self, prompt: str, **kwargs) -> dict:\\n        context_decision = self._assess_context(prompt)\\n\\n        if \\\"intuition\\\" in context_decision:\\n            response = self.intuition_block(prompt, **kwargs)\\n        elif \\\"algorithm\\\" in context_decision:\\n            response = self.algorithm_block(prompt, **kwargs)\\n        elif \\\"mixed\\\" in context_decision:\\n            response = self.debate_block(prompt, **kwargs)\\n        else:\\n            response = self.debate_block(prompt, **kwargs)  # Default strategy\\n\\n        return {\\n            \\\"content\\\": response['content'],\\n            \\\"extra\\\": {\\\"strategy\\\": context_decision}\\n        }\",\n",
      "    \"fitness\": {\n",
      "        \"95% Bootstrap Confidence Interval\": \"(40.0%, 68.0%)\",\n",
      "        \"median\": 54.0\n",
      "    }\n",
      "}\n",
      "============================== Round 4 of 15 ==============================\n",
      "Error (attempt 1/3): invalid literal for int() with base 10: 'Completeness:'\n",
      "Successfully found a new block:\n",
      "{\n",
      "    \"name\": \"DynamicCollaborativeAgent\",\n",
      "    \"thought\": \"**Insights:**\\nThe revised 'DynamicCollaborativeAgent' aims to dynamically adapt its response generation strategy in stages, integrating feedback to determine if the response should progress through debate and refinement. This allows it to potentially terminate early when confidence is high, optimizing computational resources.\\n\\n**Overall Idea:**\\nThe architecture orchestrates a process where each step evaluates the output's confidence. By incorporating a practical feedback loop, the agent can dynamically adjust the flow - moving forward, repeating, or concluding - based on objective evaluations embedded within its stages.\\n\\n**Implementation:**\\n1. Use ChainOfThought for generating initial responses.\\n2. Evaluate the confidence of that response using regex to safely parse integer confidence scores.\\n3. Proceed with MultiAgentDebate and subsequent evaluations if initial confidence is insufficient.\\n4. Final refinements occur through SelfRefine if confidence thresholds are not met post-debate.\",\n",
      "    \"code\": \"import agenticblocks as ab\\nimport re\\n\\nclass DynamicCollaborativeAgent:\\n    def __init__(self):\\n        self.model = ab.Model(\\\"openai/gpt-3.5-turbo\\\", system_prompt=\\\"You are a dynamic and adaptive assistant.\\\")\\n        self.chain_of_thought = ab.ChainOfThought(self.model)\\n        self.debate_agents = [ab.ChainOfThought(self.model) for _ in range(3)]\\n        self.multi_agent_debate = ab.MultiAgentDebate(self.debate_agents, rounds=2)\\n        self.self_refine = ab.SelfRefine(self.model, iterations=2)\\n\\n    def _should_continue(self, content):\\n        evaluation_prompt = f\\\"Evaluate the following response's completeness and correctness:\\\\n\\\\n{content}\\\\n\\\\nProvide a confidence level from 1 to 10, where 10 means the response is final and accurate.\\\"\\n        eval_result = self.model(evaluation_prompt, temperature=0)\\n        confidence_str = eval_result['content']\\n        # Extract numeric confidence measure from result\\n        matches = re.findall(r\\\"\\\\d+\\\", confidence_str)\\n        if matches:\\n            confidence_score = int(matches[0])  # Take the first detected number\\n        else:\\n            confidence_score = 0  # Default to 0 if no number detected\\n\\n        return confidence_score < 8  # Threshold to decide continuation\\n\\n    def __call__(self, prompt: str, **kwargs) -> dict:\\n        # Step 1: Chain-of-Thought generation\\n        cot_response = self.chain_of_thought(prompt, **kwargs)[\\\"content\\\"]\\n\\n        if self._should_continue(cot_response):\\n            # Step 2: Debate and potential refinement\\n            debated_response = self.multi_agent_debate(cot_response, **kwargs)[\\\"content\\\"]\\n\\n            if self._should_continue(debated_response):\\n                # Step 3: Final refinement if needed\\n                final_response = self.self_refine(debated_response, **kwargs)[\\\"content\\\"]\\n            else:\\n                final_response = debated_response\\n        else:\\n            final_response = cot_response\\n\\n        return {\\n            \\\"content\\\": final_response,\\n            \\\"extra\\\": {\\n                \\\"initial_response\\\": cot_response\\n            }\\n        }\",\n",
      "    \"fitness\": {\n",
      "        \"95% Bootstrap Confidence Interval\": \"(38.0%, 64.0%)\",\n",
      "        \"median\": 50.0\n",
      "    }\n",
      "}\n",
      "============================== Round 5 of 15 ==============================\n",
      "Successfully found a new block:\n",
      "{\n",
      "    \"name\": \"MetacognitiveReflectiveAgent\",\n",
      "    \"thought\": \"**Insights:**\\nThe architecture will enhance model performance using metacognitive layers, allowing it to dynamically assess its current state and choose actions accordingly, based on a structured evaluation of its internal decision points. \\n\\n**Overall Idea:**\\nBy integrating structured self-evaluation throughout the decision chain, the model can transition between strategies using clear indicators rather than arbitrary keyword detection. This involves deploying a scoring system to measure the depth of required further refining or debating.\\n\\n**Implementation:**\\n1. Employ a Chain of Thought to yield an initial response.\\n2. Introduce a structured self-evaluation method for reflection results, using scoring or tagging systems to understand necessity for further improvement.\\n3. Activate MultiAgentDebate if the structured reflection advises diverse inputs for better judgment.\\n4. Execute SelfRefine if the scoring suggests existing content could be improved further.\\n5. Leverage caching strategies to store reflection outcomes to minimize redundant assessment efforts.\",\n",
      "    \"code\": \"import agenticblocks as ab\\nimport re\\n\\nclass MetacognitiveReflectiveAgent:\\n    def __init__(self):\\n        self.model = ab.Model(\\\"openai/gpt-3.5-turbo\\\", system_prompt=\\\"You are a self-reflective AI model.\\\")\\n        self.chain_of_thought = ab.ChainOfThought(self.model)\\n        self.multi_agent_debate = ab.MultiAgentDebate([ab.ChainOfThought(self.model) for _ in range(3)], rounds=2)\\n        self.self_refine = ab.SelfRefine(self.model, iterations=2)\\n        self.reflection_cache = {}\\n\\n    def _self_assess(self, content):\\n        if content in self.reflection_cache:\\n            return self.reflection_cache[content]\\n        assessment_prompt = (f\\\"Reflect on the quality and completeness of the following response.\\\"\\n                             f\\\" If it can be improved or needs more perspectives, assign a reflectivity score between 0-10.\\\"\\n                             f\\\"\\\\n\\\\n{content}\\\")\\n        assessment_result = self.model(assessment_prompt, temperature=0)\\n        self.reflection_cache[content] = assessment_result['content'].strip()\\n        return self.reflection_cache[content]\\n\\n    def __call__(self, prompt: str, **kwargs) -> dict:\\n        # Step 1: Chain-of-Thought\\n        cot_response = self.chain_of_thought(prompt, **kwargs)['content']\\n        reflection = self._self_assess(cot_response)\\n        try:\\n            score = int(re.search(r\\\"\\\\d+\\\", reflection).group())\\n        except:\\n            score = 0\\n\\n        if score > 5:\\n            # Step 2: Multi-Agent Debate\\n            debated_response = self.multi_agent_debate(cot_response, **kwargs)['content']\\n            reflection = self._self_assess(debated_response)\\n            try:\\n                score = int(re.search(r\\\"\\\\d+\\\", reflection).group())\\n            except:\\n                score = 0\\n\\n            if score > 5:\\n                # Step 3: Final SelfRefine\\n                final_response = self.self_refine(debated_response, **kwargs)['content']\\n            else:\\n                final_response = debated_response\\n        else:\\n            final_response = cot_response\\n\\n        return {\\n            \\\"content\\\": final_response,\\n            \\\"extra\\\": {\\n                \\\"initial_response\\\": cot_response,\\n                \\\"final_reflection\\\": reflection\\n            }\\n        }\",\n",
      "    \"fitness\": {\n",
      "        \"95% Bootstrap Confidence Interval\": \"(14.0%, 38.0%)\",\n",
      "        \"median\": 26.0\n",
      "    }\n",
      "}\n",
      "============================== Round 6 of 15 ==============================\n",
      "Successfully found a new block:\n",
      "{\n",
      "    \"name\": \"AdaptiveHeuristicDynamicAgent\",\n",
      "    \"thought\": \"**Insights:**\\nTo advance the agent's adaptability, the revised architecture should dynamically adjust its strategy based on heuristic evaluations of the response quality regarding accuracy, completeness, and consistency. These evaluations will be integrated within a dynamic processing loop that adjusts subsequent actions based on the evaluated scores.\\n\\n**Overall Idea:** \\nThe architecture will first generate an initial response using a Chain of Thought. Followed by a heuristic evaluation to determine the quality scores on predefined metrics. Based on these scores, the architecture will dynamically decide whether to engage in knowledge retrieval or iterative refinement. The decision-making will utilize a threshold system to guide whether a deeper response processing step should be invoked.\\n\\n**Implementation:** \\n1. Implement Chain of Thought for initial response generation.\\n2. Use a detailed heuristic evaluator providing accuracy, completeness, and consistency scores.\\n3. Use adaptive threshold values to make decisions on whether further processing (knowledge retrieval or refinement) is necessary.\\n4. Integrate a caching mechanism to optimize performance and reduce redundant evaluations.\",\n",
      "    \"code\": \"import agenticblocks as ab\\nimport re\\n\\nclass AdaptiveHeuristicDynamicAgent:\\n    def __init__(self):\\n        self.model = ab.Model('openai/gpt-3.5-turbo', system_prompt='You are an adaptive heuristic evaluator and dynamic assistant.')\\n        self.chain_of_thought = ab.ChainOfThought(self.model)\\n        self.self_refine = ab.SelfRefine(self.model, iterations=3)\\n        self.knowledge_retrieval = ab.IO(self.model)\\n        self.cache = {}\\n\\n    def _heuristic_evaluate(self, content):\\n        if content in self.cache:\\n            return self.cache[content]\\n        evaluation_prompt = (\\n            f'Evaluate this response on accuracy, completeness, and consistency with scores 1-10.\\\\n\\\\n{content}')\\n        response = self.model(evaluation_prompt, temperature=0)\\n        response_scores = [int(num) for num in re.findall(r'\\\\d+', response['content'])]\\n        if len(response_scores) != 3:\\n            response_scores = [5, 5, 5]  # Default to moderate scores\\n        self.cache[content] = response_scores\\n        return response_scores\\n\\n    def __call__(self, prompt: str, **kwargs) -> dict:\\n        initial_response = self.chain_of_thought(prompt, **kwargs)['content']\\n        accuracy_score, completeness_score, consistency_score = self._heuristic_evaluate(initial_response)\\n\\n        if min(accuracy_score, completeness_score, consistency_score) < 7:\\n            knowledge_content = self.knowledge_retrieval(prompt, **kwargs)['content']\\n            improved_response = self.self_refine(f'{initial_response}\\\\nKnowledge: {knowledge_content}', **kwargs)['content']\\n            final_scores = self._heuristic_evaluate(improved_response)\\n            if min(final_scores) < 6:\\n                final_response = self.self_refine(improved_response, **kwargs)['content']\\n            else:\\n                final_response = improved_response\\n        else:\\n            final_response = initial_response\\n\\n        return {\\n            'content': final_response,\\n            'extra': {\\n                'initial_response': initial_response,\\n                'final_heuristic_scores': (accuracy_score, completeness_score, consistency_score)\\n            }\\n        }\",\n",
      "    \"fitness\": {\n",
      "        \"95% Bootstrap Confidence Interval\": \"(14.0%, 36.0%)\",\n",
      "        \"median\": 24.0\n",
      "    }\n",
      "}\n",
      "============================== Round 7 of 15 ==============================\n",
      "Error (attempt 1/3): can't multiply sequence by non-int of type 'float'\n",
      "Successfully found a new block:\n",
      "{\n",
      "    \"name\": \"ContextualWeightingAgent\",\n",
      "    \"thought\": \"**Insights:**\\nThe original architecture was supposed to dynamically assign weights to outputs based on historical task performance. However, this required operations incompatible with direct text manipulation. Instead, we can use weights to influence decision-making about the final answer. Direct concatenation or an average of votes/scores might offer a better synthesis approach.\\n\\n**Overall Idea:**\\nA revised 'ContextualWeightingAgent' should focus on gathering and synthesizing outputs by incorporating readability or confidence assessments and utilize dynamic weighting and feedback loops for behavioral improvement over time.\\n\\n**Implementation:**\\n1. Use unified techniques to gather and evaluate scores instead of direct numeric transformations on outputs.\\n2. Develop a robust system to collate text-based responses, possibly through majority agreement and score aggregation.\\n3. Refine feedback adjustment mechanisms without multiplying strings with weights to evaluate model performance for optimizing ingenuity adaptively.\",\n",
      "    \"code\": \"import agenticblocks as ab\\nfrom collections import defaultdict\\nimport re\\n\\nclass ContextualWeightingAgent:\\n    def __init__(self):\\n        self.model = ab.Model('openai/gpt-3.5-turbo', system_prompt='You are a contextual weighting assistant.')\\n        self.blocks = {\\n            'IO': ab.IO(self.model),\\n            'ChainOfThought': ab.ChainOfThought(self.model),\\n            'SelfConsistency': ab.SelfConsistency(ab.ChainOfThought(self.model), n=5, temperature=0.7),\\n            'MultiAgentDebate': ab.MultiAgentDebate([ab.ChainOfThought(self.model) for _ in range(3)], rounds=2)\\n        }\\n        self.weights = defaultdict(lambda: {key: 1.0 for key in self.blocks})  # Initialize with uniform weights\\n        self.threshold = 7  # Confidence threshold for acceptance\\n        self.cache = defaultdict(dict)\\n\\n    def _combine_outputs(self, outputs):\\n        combined_output = ' '.join([outputs[key]['content'] for key in outputs])\\n        return combined_output\\n\\n    def _evaluate_confidence(self, content):\\n        # Extract a confidence rating embedded in the result text\\n        eval_prompt = f'Evaluate this response with a confidence level (1-10): {content}'\\n        evaluation = self.model(eval_prompt, temperature=0)\\n        match = re.search(r\\\"\\\\d+\\\", evaluation['content'])\\n        score = int(match.group()) if match else 5  # Defaults to moderate confidence if not found\\n        return score\\n\\n    def _feedback_adjustment(self, prompt, score):\\n        adjustment_factor = 1.1 if score >= self.threshold else 0.9\\n        for block in self.weights[prompt]:\\n            self.weights[prompt][block] *= adjustment_factor\\n\\n    def __call__(self, prompt: str, **kwargs) -> dict:\\n        outputs = {name: block(prompt, **kwargs) for name, block in self.blocks.items()}\\n        combined = self._combine_outputs(outputs)\\n        confidence_score = self._evaluate_confidence(combined)\\n        self._feedback_adjustment(prompt, confidence_score)\\n\\n        self.cache[prompt]['last_score'] = confidence_score\\n        return {\\n            'content': combined if confidence_score >= self.threshold else 'Further refinement needed.',\\n            'extra': {'raw_outputs': outputs, 'confidence_score': confidence_score}\\n        }\",\n",
      "    \"fitness\": {\n",
      "        \"95% Bootstrap Confidence Interval\": \"(44.0%, 72.0%)\",\n",
      "        \"median\": 58.0\n",
      "    }\n",
      "}\n",
      "============================== Round 8 of 15 ==============================\n",
      "Successfully found a new block:\n",
      "{\n",
      "    \"name\": \"DynamicRoleReflector\",\n",
      "    \"thought\": \"**Insights:**\\nEnhancing the reflective and heuristic learning approach with an improved dynamic feedback system will boost the accuracy. We'll incorporate a more active memory system and learning from past responses to make decisions.\\n\\n**Overall Idea:**\\nThe new architecture 'DynamicRoleReflector' will retain the diverse role displays but emphasize dynamic learning through refining roles and perspectives based on past performance. It will blend an ensemble approach with memory storage, and criteria-based adaptation, focusing on strengths derived through observed performance; integrating a feedback mechanism that can adjust role emphasis in future iterations.\\n\\n**Implementation:**\\n1. Use ChainOfThought agents representing different roles.\\n2. Implement an evaluative mechanism assessing each role's input in completeness and clarity, adjusting role emphasis dynamically.\\n3. Design a learning memory mechanism storing past performance data and learned role effectiveness, adapting to improve over time.\\n4. Use a SelfRefine and consensus-based generation step for high-quality final output.\",\n",
      "    \"code\": \"import agenticblocks as ab\\nimport re\\nfrom collections import defaultdict\\n\\nclass DynamicRoleReflector:\\n    def __init__(self):\\n        self.model = ab.Model(\\\"openai/gpt-3.5-turbo\\\", system_prompt=\\\"You are a reflector for adaptive role-based assessment.\\\")\\n        self.roles = {\\n            'Scientist': ab.ChainOfThought(self.model, template=\\\"As a Scientist: {prompt}. Let's think critically.\\\"),\\n            'Historian': ab.ChainOfThought(self.model, template=\\\"As a Historian: {prompt}. Let's analyze carefully.\\\"),\\n            'Philosopher': ab.ChainOfThought(self.model, template=\\\"As a Philosopher: {prompt}. Let's contemplate deeply.\\\")\\n        }\\n        self.self_refine = ab.SelfRefine(self.model, iterations=3)\\n        self.performance_memory = defaultdict(lambda: [5, 5, 5])  # Initialize role scores\\n\\n    def _evaluate_perspectives(self, insights):\\n        evaluations = {}\\n        for role, insight in insights.items():\\n            eval_prompt = (\\\"Evaluate the given role output for clarity, depth, and insight on a 1 to 10 scale each\\\\n\\\"\\n                           + insight)\\n            response = self.model(eval_prompt, temperature=0)\\n            scores = [int(num) for num in re.findall(r'\\\\d+', response['content'])]\\n            if len(scores) == 3:\\n                evaluations[role] = scores\\n            else:\\n                evaluations[role] = [5, 5, 5]  # Default moderate scores\\n        return evaluations\\n\\n    def _adjust_based_on_performance(self, evaluations):\\n        for role in evaluations:\\n            avg_prev = self.performance_memory[role]\\n            current_scores = evaluations[role]\\n            self.performance_memory[role] = [(o + n) / 2 for o, n in zip(avg_prev, current_scores)]\\n            self.roles[role].weight_modifier = sum(current_scores) / 30\\n\\n    def __call__(self, prompt: str, **kwargs) -> dict:\\n        insights = {role: self.roles[role](prompt, **kwargs)['content'] for role in self.roles}\\n        evaluations = self._evaluate_perspectives(insights)\\n        self._adjust_based_on_performance(evaluations)\\n        combined_response = \\\" \\\".join(insights.values())\\n        consensus = self.self_refine(combined_response, **kwargs)\\n\\n        return {\\n            \\\"content\\\": consensus['content'],\\n            \\\"extra\\\": {\\n                \\\"role_insights\\\": insights,\\n                \\\"evaluations\\\": evaluations\\n            }\\n        }\",\n",
      "    \"fitness\": {\n",
      "        \"95% Bootstrap Confidence Interval\": \"(2.0%, 18.0%)\",\n",
      "        \"median\": 10.0\n",
      "    }\n",
      "}\n",
      "============================== Round 9 of 15 ==============================\n",
      "Error (attempt 1/3): Expecting ',' delimiter: line 8 column 15 (char 2143)\n",
      "Successfully found a new block:\n",
      "{\n",
      "    \"name\": \"AutomatedFeedbackAgent\",\n",
      "    \"thought\": \"**Insights:**\\nAutomating feedback using internal model evaluations can simulate user feedback effectively, reducing reliance on manual input and enhancing efficiency. This approach holds value due to its simulated feedback mechanisms, optimizing response strategies based on heuristic evaluations.\\n\\n**Overall Idea:**\\nIntroduce an 'AutomatedFeedbackAgent' that replaces manual feedback with automated model evaluations. This agent will use its evaluations of response quality to adjust strategies dynamically, improving performance over time.\\n\\n**Implementation:**\\n1. Generate initial responses with a ChainOfThought block.\\n2. Evaluate response quality automatically using heuristic scoring for accuracy, completeness, and clarity.\\n3. Based on scores, dynamically refine responses via SelfRefine and achieve consensus through MultiAgentDebate.\\n4. Adapt strategies over time using stored performance data, simulating human-like iterative feedback.\",\n",
      "    \"code\": \"import agenticblocks as ab\\nimport re\\nfrom collections import defaultdict\\n\\nclass AutomatedFeedbackAgent:\\n    def __init__(self):\\n        self.model = ab.Model(\\\"openai/gpt-3.5-turbo\\\", system_prompt=\\\"You are an automated feedback-driven assistant.\\\")\\n        self.chain_of_thought = ab.ChainOfThought(self.model)\\n        self.self_consistency = ab.SelfConsistency(ab.ChainOfThought(self.model), n=5, temperature=0.7)\\n        self.multi_agent_debate = ab.MultiAgentDebate([ab.ChainOfThought(self.model) for _ in range(3)], rounds=2)\\n        self.performance_memory = defaultdict(lambda: [5, 5, 5])\\n\\n    def _evaluate_response(self, content):\\n        evaluation_prompt = (f'Evaluate the response for accuracy, completeness, and clarity with scores from 1 to 10.\\\\n\\\\n{content}')\\n        response = self.model(evaluation_prompt, temperature=0)\\n        scores = [int(num) for num in re.findall(r'\\\\d+', response['content'])]\\n        if len(scores) != 3:\\n            scores = [5, 5, 5]  # Default to moderate scores if evaluation fails\\n        return scores\\n\\n    def __call__(self, prompt: str, **kwargs) -> dict:\\n        initial_response = self.chain_of_thought(prompt, **kwargs)['content']\\n        feedback_scores = self._evaluate_response(initial_response)\\n\\n        if min(feedback_scores) < 7:\\n            refined_response = self.self_consistency(initial_response, **kwargs)['content']\\n            debated_response = self.multi_agent_debate(refined_response, **kwargs)['content']\\n            final_response = debated_response\\n            self.performance_memory[prompt] = feedback_scores\\n        else:\\n            final_response = initial_response\\n\\n        return {\\n            'content': final_response,\\n            'extra': {\\n                'initial_response': initial_response,\\n                'feedback_scores': feedback_scores\\n            }\\n        }\",\n",
      "    \"fitness\": {\n",
      "        \"95% Bootstrap Confidence Interval\": \"(12.0%, 34.0%)\",\n",
      "        \"median\": 22.0\n",
      "    }\n",
      "}\n",
      "============================== Round 10 of 15 ==============================\n",
      "Error (attempt 1/3): Expecting ',' delimiter: line 8 column 15 (char 2430)\n",
      "Successfully found a new block:\n",
      "{\n",
      "    \"name\": \"ReasonedVerifierAgent\",\n",
      "    \"thought\": \"**Insights:** \\nIntegrating factual verification can push architecture boundaries, though practical challenges exist. A credible improvement involves simulating factual verification processes, ensuring factual integrity feasibly with consideration for dynamic adaptability.\\n\\n**Overall Idea:**\\n'ReasonedVerifierAgent' will simulate fact-checking while preserving logical soundness. The architecture aims for factual integrity in responses by combining simulated database verification processes, hypothetical API call stubs, and progressive logical exploration.\\n\\n**Implementation:**\\n1. Use ChainOfThought for stepwise reasoning in primary response generation.\\n2. Implement `MockFactualVerifier` to simulate factual checks with predefined responses or conditions.\\n3. Develop a coherence and factuality evaluation, refining responses upon failure without needing real API logic.\\n4. Utilize SelfRefine to reinforce response reliability and streamline knowledge consolidation processes seamlessly.\",\n",
      "    \"code\": \"import agenticblocks as ab\\nimport re\\n\\nclass ReasonedVerifierAgent:\\n    def __init__(self):\\n        self.model = ab.Model(\\\"openai/gpt-3.5-turbo\\\", system_prompt=\\\"You are a knowledgeable assistant that values factual accuracy.\\\")\\n        self.chain_of_thought = ab.ChainOfThought(self.model)\\n        self.self_refine = ab.SelfRefine(self.model, iterations=2)\\n\\n    def _mock_factual_verification(self, content):\\n        # Simulated factual verification logic\\n        mock_response = \\\"Factual Content Seems Mostly Correct\\\" if \\\"Cassiopeia\\\" in content else \\\"Potential Factual Inaccuracy Detected\\\"\\n        score = 9 if \\\"Cassiopeia\\\" in content else 5\\n        return score, mock_response\\n\\n    def _evaluate_decision(self, content):\\n        score, feedback = self._mock_factual_verification(content)\\n        if score >= 8:\\n            return content, True  # Trustworthy response\\n        return feedback, False  # Needs refining\\n\\n    def __call__(self, prompt: str, **kwargs) -> dict:\\n        initial_response = self.chain_of_thought(prompt, **kwargs)['content']\\n        evaluated_response, is_verified = self._evaluate_decision(initial_response)\\n\\n        if not is_verified:\\n            refined_response = self.self_refine(evaluated_response, **kwargs)['content']\\n            final_response = refined_response\\n        else:\\n            final_response = evaluated_response\\n\\n        return {\\n            'content': final_response,\\n            'extra': {\\n                'initial_response': initial_response,\\n                'evaluation_feedback': evaluated_response\\n            }\\n        }\",\n",
      "    \"fitness\": {\n",
      "        \"95% Bootstrap Confidence Interval\": \"(0.0%, 0.0%)\",\n",
      "        \"median\": 0.0\n",
      "    }\n",
      "}\n",
      "============================== Round 11 of 15 ==============================\n",
      "Error (attempt 1/3): 'NoneType' object has no attribute 'group'\n",
      "Successfully found a new block:\n",
      "{\n",
      "    \"name\": \"AdaptiveContextualRoleAgent\",\n",
      "    \"thought\": \"**Insights:**\\nThe architecture relies on effective feedback and role assessment using contextual evaluations to guide weight assignment, focusing on dynamic synthesis from specialized roles. To ensure robustness, feedback evaluations must properly handle cases where expected patterns (like numeric scores) aren't found in responses.\\n\\n**Overall Idea:**\\nThe main aim is maintaining adaptability by assessing the task context and then dynamically weighting different roles accordingly. Roles are specialized rational, creative, and factual agents. To ensure proper weight assignment and improvement tracking, it's essential to seamlessly and safely evaluate the feedback.\\n\\n**Implementation:**\\n1. **Contextual Assessment:** Determine the task's nature (factual, logical, creative) using a prompt-based query mechanism. \\n2. **Role Evaluation and Weighting:** Assign initial weights informed by context evaluation, then dynamically adjust through iterative runs based on feedback.\\n3. **Synthesis:** Combine role outputs based on calculated weights.\\n4. **Robust Evaluation Feedback:** Use regex safely to extract numeric evaluations, employing defaults where needed to avoid runtime errors.\\n5. **Improvement and Refinement:** Use role feedback and role caching to drive continued improvement of weight dynamics and synthesis outputs.\",\n",
      "    \"code\": \"import agenticblocks as ab\\nfrom collections import defaultdict\\nimport re\\n\\nclass AdaptiveContextualRoleAgent:\\n    def __init__(self):\\n        self.model = ab.Model(\\\"openai/gpt-3.5-turbo\\\", system_prompt=\\\"You are an adaptive role-weighting assistant based on contextual assessment.\\\")\\n        self.roles = {\\n            'Factual Expert': ab.ChainOfThought(self.model, template=\\\"As a factual expert: {prompt}\\\\nLet's verify the facts.\\\"),\\n            'Logical Analyst': ab.ChainOfThought(self.model, template=\\\"As a logical analyst: {prompt}\\\\nLet's reason logically.\\\"),\\n            'Creative Thinker': ab.ChainOfThought(self.model, template=\\\"As a creative thinker: {prompt}\\\\nLet's explore creative ideas.\\\")\\n        }\\n        self.self_refine = ab.SelfRefine(self.model, iterations=2)\\n        self.performance_memory = defaultdict(list)\\n\\n    def _contextual_assessment(self, prompt):\\n        assessment_prompt = f\\\"Assess this question for its context - Determine if it requires a factual, logical, or creative approach:\\\\n\\\\n{prompt}\\\"\\n        result = self.model(assessment_prompt, temperature=0.5)\\n        return result['content'].strip().lower()\\n\\n    def _assign_weights(self, context):\\n        weights = {role: 1.0 for role in self.roles}\\n        if 'factual' in context:\\n            weights['Factual Expert'] = 1.5\\n        if 'logical' in context:\\n            weights['Logical Analyst'] = 1.5\\n        if 'creative' in context:\\n            weights['Creative Thinker'] = 1.5\\n        return weights\\n\\n    def _synthesize_response(self, weights, role_outputs):\\n        return ' '.join([output for role, output in role_outputs.items() for _ in range(int(weights[role]))])\\n\\n    def _evaluate_feedback(self, prompt, role_outputs):\\n        feedback = {}\\n        for role, output in role_outputs.items():\\n            eval_prompt = f\\\"Evaluate correctness and relevance of this role output: {output}\\\"\\n            try:\\n                res = self.model(eval_prompt, temperature=0)\\n                score = int(re.search(r\\\"\\\\d+\\\", res['content']).group()) if re.search(r\\\"\\\\d+\\\", res['content']) else 5\\n            except:\\n                score = 5\\n            feedback[role] = score\\n            self.performance_memory[prompt].append((role, score))\\n        return feedback\\n\\n    def __call__(self, prompt: str, **kwargs) -> dict:\\n        context = self._contextual_assessment(prompt)\\n        weights = self._assign_weights(context)\\n        role_outputs = {role: self.roles[role](prompt, **kwargs)['content'] for role in self.roles}\\n        synthesized_output = self._synthesize_response(weights, role_outputs)\\n        refined_response = self.self_refine(synthesized_output, **kwargs)['content']\\n        feedback = self._evaluate_feedback(prompt, role_outputs)\\n\\n        return {\\n            \\\"content\\\": refined_response,\\n            \\\"extra\\\": {\\n                \\\"context\\\": context,\\n                \\\"role_outputs\\\": role_outputs,\\n                \\\"feedback\\\": feedback,\\n                \\\"weights\\\": weights\\n            }\\n        }\",\n",
      "    \"fitness\": {\n",
      "        \"95% Bootstrap Confidence Interval\": \"(4.0%, 22.0%)\",\n",
      "        \"median\": 12.0\n",
      "    }\n",
      "}\n",
      "============================== Round 12 of 15 ==============================\n",
      "Error (attempt 1/3): No module named 'sentence_transformers'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:15: RuntimeWarning: invalid value encountered in scalar divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully found a new block:\n",
      "{\n",
      "    \"name\": \"SemanticKnowledgeAugmentedAgent\",\n",
      "    \"thought\": \"**Insights:**\\nThe integration of semantic similarity for knowledge retrieval can substantially enrich language models by deriving contextually relevant information, but relying on external libraries such as `sentence_transformers` introduces dependency issues. Addressing this, we can design an internal mechanism for semantic checking using cosine similarity on manually defined embeddings or keywords.\\n\\n**Overall Idea:**\\nTo develop a self-contained semantic similarity process that does not depend on external libraries, a pre-defined list of keywords will be used to simulate semantic augmentation. This results in a simpler but integrated agent, providing more robust responses by broadening its informational scope.\\n\\n**Implementation:**\\n1. Define a simple method to compare semantic relevance between prompt keywords and the knowledge base without external dependencies.\\n2. Implement cosine similarity checks with manual keyword handling for document retrieval.\\n3. Integrate retrieved snippets and refine the result with `SelfRefine` for a polished outcome.\",\n",
      "    \"code\": \"import agenticblocks as ab\\nimport numpy as np\\n\\nclass SemanticKnowledgeAugmentedAgent:\\n    def __init__(self):\\n        self.model = ab.Model('openai/gpt-3.5-turbo', system_prompt='You are a semantic-enhanced assistant.')\\n        self.chain_of_thought = ab.ChainOfThought(self.model)\\n        self.self_refine = ab.SelfRefine(self.model, iterations=2)\\n        self.knowledge_base = {\\n            'constellation': 'Cassiopeia is a constellation in the northern sky, known for its W-shaped appearance.',\\n            'astronomy': 'Astronomy is the study of celestial objects, phenomena, and the universe as a whole.'\\n        }\\n\\n    def _basic_cosine_similarity(self, vec1, vec2):\\n        return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\\n\\n    def _semantic_information_retrieval(self, prompt):\\n        words_freq = {word: prompt.lower().split().count(word) for word in self.knowledge_base.keys()}\\n        similarities = {k: self._basic_cosine_similarity(np.array(list(words_freq.values())), np.array([1, 1])) for k in self.knowledge_base}\\n        best_match = max(similarities, key=similarities.get)\\n        return self.knowledge_base[best_match] if similarities[best_match] > 0.1 else ''\\n\\n    def __call__(self, prompt: str, **kwargs) -> dict:\\n        # Step 1: Generate initial thought\\n        initial_response = self.chain_of_thought(prompt, **kwargs)['content']\\n        \\n        # Step 2: Retrieve additional knowledge based on semantic matching\\n        retrieved_knowledge = self._semantic_information_retrieval(prompt)\\n\\n        # Step 3: Combine into a refined response\\n        combined_content = f'{initial_response} Knowledge: {retrieved_knowledge}'\\n        refined_response = self.self_refine(combined_content, **kwargs)['content']\\n\\n        return {\\n            'content': refined_response,\\n            'extra': {\\n                'initial_response': initial_response,\\n                'retrieved_knowledge': retrieved_knowledge\\n            }\\n        }\",\n",
      "    \"fitness\": {\n",
      "        \"95% Bootstrap Confidence Interval\": \"(4.0%, 24.0%)\",\n",
      "        \"median\": 14.0\n",
      "    }\n",
      "}\n",
      "============================== Round 13 of 15 ==============================\n",
      "Successfully found a new block:\n",
      "{\n",
      "    \"name\": \"AdaptiveMetaCognitionAgent\",\n",
      "    \"thought\": \"**Insights:**\\nThe 'AdaptiveMetaCognitionAgent' aims to better capture the essence of dynamic strategy selection and adaptability through a feedback-driven approach, using memory to shape future decisions based on successful outcomes in similar past scenarios.\\n\\n**Overall Idea:**\\nThis architecture refines the approach by focusing more on leveraging both learning and adaptability. The agent will analyze domain and question type, updating its cache with the efficacy of each strategy for similar past questions, and iteratively refining outputs based on feedback and historical data.\\n\\n**Implementation:**\\n1. **Enhanced Context Analysis:** Implement a more sophisticated context analysis process that categorizes questions into multiple roles actively leveraging both initial input and past successful strategies.\\n2. **Dynamic Memory System:** Use an advanced memory system that records successful strategies and role interplay outcomes from past efforts, adjusting role weights dynamically based on these insights.\\n3. **Refined Feedback Loop:** Incorporate a feedback loop not just measuring success but retraining the strategy selection mechanism where consistent underperformance is detected.\\n4. **Adaptive Strategy Application:** Create a scoring system for each role and strategy to quantitatively assess performance and inform ongoing role based logic adaptation.\",\n",
      "    \"code\": \"import agenticblocks as ab\\nfrom collections import defaultdict\\nimport re\\n\\nclass AdaptiveMetaCognitionAgent:\\n    def __init__(self):\\n        self.model = ab.Model(\\\"openai/gpt-3.5-turbo\\\", system_prompt=\\\"You are a highly adaptive AI model.\\\")\\n        self.chain_of_thought = ab.ChainOfThought(self.model)\\n        self.roles = {\\n            'Logical Analyst': ab.ChainOfThought(self.model, template=\\\"As a logical analyst: {prompt}\\\\nLet's reason logically.\\\"),\\n            'Creative Thinker': ab.ChainOfThought(self.model, template=\\\"As a creative thinker: {prompt}\\\\nLet's explore creative ideas.\\\")\\n        }\\n        self.strategies = {\\n            'Chain': ab.ChainOfThought(self.model),\\n            'Debate': ab.MultiAgentDebate([ab.ChainOfThought(self.model) for _ in range(3)], rounds=2),\\n            'Refine': ab.SelfRefine(self.model, iterations=3)\\n        }\\n        self.memory = defaultdict(lambda: {'success': 0, 'total': 0})  # Track strategy successes\\n        self.threshold = 6\\n\\n    def _analyze_context(self, prompt):\\n        analysis_prompt = f\\\"Analyze context of the question: {prompt}\\\"\\n        result = self.chain_of_thought.model(analysis_prompt, temperature=0.5)\\n        return result['content'].strip().lower()\\n\\n    def _evaluate_output(self, content):\\n        eval_prompt = f\\\"Review this response for accuracy and clarity on a scale of 1 to 10: {content}\\\"\\n        evaluation = self.model(eval_prompt, temperature=0)\\n        match = re.search(r\\\"\\\\d+\\\", evaluation['content'])\\n        return int(match.group()) if match else 5\\n\\n    def __call__(self, prompt: str, **kwargs) -> dict:\\n        context = self._analyze_context(prompt)\\n        scores = {}\\n\\n        for strategy_name, strategy_block in self.strategies.items():\\n            success_rate = self.memory[strategy_name]['success'] / max(1, self.memory[strategy_name]['total'])\\n            attempt_content = strategy_block(prompt, **kwargs)['content']\\n            score = self._evaluate_output(attempt_content) + success_rate\\n            scores[strategy_name] = score\\n            if score > self.threshold:\\n                self.memory[strategy_name]['success'] += 1\\n            self.memory[strategy_name]['total'] += 1\\n\\n        best_strategy = max(scores, key=scores.get)\\n        final_output = self.strategies[best_strategy](prompt, **kwargs)['content']\\n\\n        return {\\n            'content': final_output,\\n            'extra': {\\n                'context': context,\\n                'best_strategy': best_strategy,\\n                'scores': scores\\n            }\\n        }\",\n",
      "    \"fitness\": {\n",
      "        \"95% Bootstrap Confidence Interval\": \"(38.0%, 64.0%)\",\n",
      "        \"median\": 52.0\n",
      "    }\n",
      "}\n",
      "============================== Round 14 of 15 ==============================\n",
      "Successfully found a new block:\n",
      "{\n",
      "    \"name\": \"AdaptiveWeightedConsensusAgent\",\n",
      "    \"thought\": \"**Insights:**\\nWhile existing ensemble-like architectures with multi-agent debate strategies are prevalent, enhancing decision fusion with intricate weight adjustments and dynamic role adaptability\\u2014using context evaluations and precedent decision efficiencies\\u2014can further refine results. We reflect on the importance of leveraging a dynamic, adaptive weighting system that recalculates based on empirical feedback loop mechanisms.\\n\\n**Overall Idea:**\\nThe revised 'AdaptiveWeightedConsensusAgent' focuses on improving the voting mechanism where contextually driven weights influence the consensus decision. Roles contribute differently based on previous efficacies and gain current evaluation scaling within their strategic utility framework, enhancing overall decision integrity.\\n\\n**Implementation:**\\n1. Implement a complex weighting mechanism for each role influenced by evaluated response confidence scores, dynamically adjusted.\\n2. Use a context learner to adapt role priorities dynamically for voting recalibrations, scaled on prompt efficiency metrics.\\n3. Retain a memory structure not only tracking performance feedback but consistently reevaluating contribution levels per strategic role.\\n4. Incorporate iterative weighting recalibration based on prompt relevancy and success feedback, assuring roles' priority alignment with current contexts.\",\n",
      "    \"code\": \"import agenticblocks as ab\\nimport re\\nfrom collections import defaultdict\\n\\nclass AdaptiveWeightedConsensusAgent:\\n    def __init__(self):\\n        self.model = ab.Model('openai/gpt-3.5-turbo', system_prompt='You are an adaptive consensus assistant.')\\n        self.intuition_agent = ab.ChainOfThought(self.model)\\n        self.algorithm_agent = ab.SelfConsistency(ab.IO(self.model), n=5, temperature=0.7)\\n        self.debate_agent = ab.MultiAgentDebate([ab.ChainOfThought(self.model) for _ in range(3)], rounds=2)\\n        self.voting_memory = defaultdict(list)  # Monitor role performance\\n\\n    def _evaluate_confidence(self, content):\\n        eval_prompt = f'Evaluate this response with a confidence level (1-10): {content}'\\n        evaluation = self.model(eval_prompt, temperature=0)\\n        match = re.search(r\\\"\\\\d+\\\", evaluation['content'])\\n        return int(match.group()) if match else 5\\n\\n    def _adjust_weights(self, prompt, confidence_scores):\\n        # Adjust weights dynamically based on past success rates\\n        total_confidence = sum(confidence_scores.values())\\n        weights = {role: score / total_confidence for role, score in confidence_scores.items()}\\n        return weights\\n\\n    def _aggregate_responses(self, responses, weights):\\n        # Aggregate weighted responses\\n        aggregate_score = defaultdict(float)\\n        for role, response in responses.items():\\n            aggregate_score[response] += weights[role]\\n        return max(aggregate_score, key=aggregate_score.get)\\n\\n    def __call__(self, prompt: str, **kwargs) -> dict:\\n        # Collect responses from different agents\\n        responses = {\\n            'intuition': self.intuition_agent(prompt, **kwargs)['content'],\\n            'algorithm': self.algorithm_agent(prompt, **kwargs)['content'],\\n            'debate': self.debate_agent(prompt, **kwargs)['content']\\n        }\\n\\n        # Evaluate confidence\\n        confidence_scores = {agent: self._evaluate_confidence(responses[agent]) for agent in responses}\\n        weights = self._adjust_weights(prompt, confidence_scores)\\n\\n        # Aggregate responses\\n        final_response = self._aggregate_responses(responses, weights)\\n\\n        # Store memory\\n        for agent, score in confidence_scores.items():\\n            self.voting_memory[prompt].append(score)\\n\\n        return {\\n            'content': final_response,\\n            'extra': {\\n                'raw_responses': responses,\\n                'confidence_scores': confidence_scores,\\n                'weights': weights\\n            }\\n        }\",\n",
      "    \"fitness\": {\n",
      "        \"95% Bootstrap Confidence Interval\": \"(52.0%, 78.0%)\",\n",
      "        \"median\": 64.0\n",
      "    }\n",
      "}\n",
      "============================== Round 15 of 15 ==============================\n",
      "Successfully found a new block:\n",
      "{\n",
      "    \"name\": \"ContextualMemoryEnhancedAgent\",\n",
      "    \"thought\": \"**Insights:**\\nRefining memory usage can be central to improving adaptability in dynamic strategy selection. By making memory dependent on contextual evaluation and specific scenario engagement, more sophisticated adjustments could be integrated where roles not only supply content but also impact strategic adjustments dynamically.\\n\\n**Overall Idea:**\\nThe 'ContextualMemoryEnhancedAgent' will form autonomous, contextual memory banks based on learned experiences and question types, actively engaging these memories in strategy determination across roles. This includes associating past feedback with similar questions and incorporating weighted role outputs in strategic weight calculation.\\n\\n**Implementation:**\\n1. Use specialized contextual memory banks and strategy wrappers to dynamically adapt strategy use based on contextual analysis and memory banks.\\n2. Assign roles to perform and supply evidence and exploratory content independently, with historical memory playing a part in strategy enhancement.\\n3. Implement a strategic balance where feedback is retrospectively applied across stored memories, enabling broader adaptive change alongside individual strategy adjustment when needed.\",\n",
      "    \"code\": \"import agenticblocks as ab\\nfrom collections import defaultdict\\nimport re\\n\\nclass ContextualMemoryEnhancedAgent:\\n    def __init__(self):\\n        self.model = ab.Model(\\\"openai/gpt-3.5-turbo\\\", system_prompt=\\\"You are a contextual memory-enhanced assistant.\\\")\\n        self.memory = defaultdict(lambda: {'success': 0, 'attempts': 0, 'contexts': []})\\n        self.roles = {\\n            'Factual Expert': ab.ChainOfThought(self.model, template=\\\"As a factual expert: {prompt}\\\\nLet's verify the facts.\\\"),\\n            'Creative Thinker': ab.ChainOfThought(self.model, template=\\\"As a creative thinker: {prompt}\\\\nLet's explore creative ideas.\\\")\\n        }\\n        self.strategies = [\\n            ('SimpleThink', ab.IO(self.model)),\\n            ('InDepthAnalysis', ab.ChainOfThought(self.model)),\\n            ('CollaborativeDebate', ab.MultiAgentDebate([ab.ChainOfThought(self.model) for _ in range(3)], rounds=2))\\n        ]\\n        self.threshold = 7\\n\\n    def _context_analysis(self, prompt):\\n        analysis_prompt = f\\\"Evaluate the context of the following task: {prompt}\\\"\\n        result = self.model(analysis_prompt, temperature=0.5)\\n        return result['content'].strip().lower()\\n\\n    def _dynamic_strategy_selection(self, context):\\n        context_breakdown = context.split()\\n        influence_scores = {strategy_name: sum(context_breakdown.count(c) for c in self.memory[strategy_name]['contexts']) for strategy_name, _ in self.strategies}\\n        scores = {strategy_name: (self.memory[strategy_name]['success'] / max(1, self.memory[strategy_name]['attempts']) + influence) for strategy_name, influence in influence_scores.items()}\\n        return max(scores, key=scores.get)\\n\\n    def __call__(self, prompt: str, **kwargs) -> dict:\\n        context = self._context_analysis(prompt)\\n        chosen_strategy_name = self._dynamic_strategy_selection(context)\\n\\n        chosen_strategy = dict(self.strategies)[chosen_strategy_name]\\n        response_dict = chosen_strategy(prompt, **kwargs)\\n        response_content = response_dict['content']\\n\\n        feedback = self.model(f\\\"Rate this response's accuracy and clarity (1-10): {response_content}\\\\nBased on similar previous interactions: {context}\\\", temperature=0)\\n        score_match = re.search(r\\\"\\\\d+\\\", feedback['content'])\\n        score = int(score_match.group()) if score_match else 5\\n\\n        # Storing attempts and updating memory\\n        self.memory[chosen_strategy_name]['attempts'] += 1\\n        self.memory[chosen_strategy_name]['contexts'].extend(context.split())\\n        if score >= self.threshold:\\n            self.memory[chosen_strategy_name]['success'] += 1\\n\\n        return {\\n            'content': response_content,\\n            'extra': {\\n                'chosen_strategy': chosen_strategy_name,\\n                'context': context\\n            }\\n        }\",\n",
      "    \"fitness\": {\n",
      "        \"95% Bootstrap Confidence Interval\": \"(44.0%, 72.0%)\",\n",
      "        \"median\": 58.0\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "n_iters = 15\n",
    "for iter in range(n_iters):\n",
    "    print(\"=\"*30, f\"Round {iter+1} of {n_iters}\", \"=\"*30)\n",
    "    meta_agent.reset_history()  # reset the models message history\n",
    "    response = meta_agent(prompt.replace(\"[ARCHIVE]\", json.dumps(archive)))\n",
    "    response = meta_agent(reflection_template.format(previous_response=json.dumps(response)))\n",
    "\n",
    "    success = False\n",
    "    for retry in range(5):\n",
    "        try:\n",
    "            # Parse JSON response\n",
    "            response_parsed = json.loads(response[\"content\"].split('```json\\n')[-1].split('\\n```')[0])\n",
    "            \n",
    "            # Review mode: show code and ask for approval\n",
    "            if not YOLO:\n",
    "                print(f\"\\n{response_parsed['name']}:\\n{response_parsed['code']}\\n\")\n",
    "                if input(\"Run? [Y/n]: \").strip().lower() == 'n':\n",
    "                    break\n",
    "            \n",
    "            # Execute the code\n",
    "            namespace = globals().copy()\n",
    "            exec(response_parsed[\"code\"], namespace)\n",
    "            NewBlock = namespace[response_parsed[\"name\"]]\n",
    "            block = NewBlock()\n",
    "            ci_lo, ci_hi, med = eval_block(block, df)\n",
    "            success = True\n",
    "            break\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error (attempt {retry + 1}/3): {e}\")\n",
    "            response = meta_agent(f\"The following exception occurred when trying to test the block: {e}. Traceback: {traceback.format_exc()}. Return a valid JSON with keys name, code, thought and reflection. The name is the class name of the found block, code contains the fixed implementation of the block, thought as before the thought process behind the block design and reflection your reflection on the exception.\")\n",
    "\n",
    "    if success:\n",
    "        result = {\n",
    "            \"name\": NewBlock.__name__,\n",
    "            \"thought\": response_parsed[\"thought\"],\n",
    "            \"code\": response_parsed[\"code\"],\n",
    "            \"fitness\": {\"95% Bootstrap Confidence Interval\": f\"({ci_lo*100:.1f}%, {ci_hi*100:.1f}%)\", \"median\": round(med*100, 1)},\n",
    "        }\n",
    "        archive += [result]\n",
    "        print(\"Successfully found a new block:\")\n",
    "        print(json.dumps(result, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dbf17c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaptiveWeightedConsensusAgent - Score:  64.0\n",
      "\n",
      "**Insights:**\n",
      "While existing ensemble-like architectures with multi-agent debate strategies are prevalent, enhancing decision fusion with intricate weight adjustments and dynamic role adaptability—using context evaluations and precedent decision efficiencies—can further refine results. We reflect on the importance of leveraging a dynamic, adaptive weighting system that recalculates based on empirical feedback loop mechanisms.\n",
      "\n",
      "**Overall Idea:**\n",
      "The revised 'AdaptiveWeightedConsensusAgent' focuses on improving the voting mechanism where contextually driven weights influence the consensus decision. Roles contribute differently based on previous efficacies and gain current evaluation scaling within their strategic utility framework, enhancing overall decision integrity.\n",
      "\n",
      "**Implementation:**\n",
      "1. Implement a complex weighting mechanism for each role influenced by evaluated response confidence scores, dynamically adjusted.\n",
      "2. Use a context learner to adapt role priorities dynamically for voting recalibrations, scaled on prompt efficiency metrics.\n",
      "3. Retain a memory structure not only tracking performance feedback but consistently reevaluating contribution levels per strategic role.\n",
      "4. Incorporate iterative weighting recalibration based on prompt relevancy and success feedback, assuring roles' priority alignment with current contexts.\n",
      "\n",
      "import agenticblocks as ab\n",
      "import re\n",
      "from collections import defaultdict\n",
      "\n",
      "class AdaptiveWeightedConsensusAgent:\n",
      "    def __init__(self):\n",
      "        self.model = ab.Model('openai/gpt-3.5-turbo', system_prompt='You are an adaptive consensus assistant.')\n",
      "        self.intuition_agent = ab.ChainOfThought(self.model)\n",
      "        self.algorithm_agent = ab.SelfConsistency(ab.IO(self.model), n=5, temperature=0.7)\n",
      "        self.debate_agent = ab.MultiAgentDebate([ab.ChainOfThought(self.model) for _ in range(3)], rounds=2)\n",
      "        self.voting_memory = defaultdict(list)  # Monitor role performance\n",
      "\n",
      "    def _evaluate_confidence(self, content):\n",
      "        eval_prompt = f'Evaluate this response with a confidence level (1-10): {content}'\n",
      "        evaluation = self.model(eval_prompt, temperature=0)\n",
      "        match = re.search(r\"\\d+\", evaluation['content'])\n",
      "        return int(match.group()) if match else 5\n",
      "\n",
      "    def _adjust_weights(self, prompt, confidence_scores):\n",
      "        # Adjust weights dynamically based on past success rates\n",
      "        total_confidence = sum(confidence_scores.values())\n",
      "        weights = {role: score / total_confidence for role, score in confidence_scores.items()}\n",
      "        return weights\n",
      "\n",
      "    def _aggregate_responses(self, responses, weights):\n",
      "        # Aggregate weighted responses\n",
      "        aggregate_score = defaultdict(float)\n",
      "        for role, response in responses.items():\n",
      "            aggregate_score[response] += weights[role]\n",
      "        return max(aggregate_score, key=aggregate_score.get)\n",
      "\n",
      "    def __call__(self, prompt: str, **kwargs) -> dict:\n",
      "        # Collect responses from different agents\n",
      "        responses = {\n",
      "            'intuition': self.intuition_agent(prompt, **kwargs)['content'],\n",
      "            'algorithm': self.algorithm_agent(prompt, **kwargs)['content'],\n",
      "            'debate': self.debate_agent(prompt, **kwargs)['content']\n",
      "        }\n",
      "\n",
      "        # Evaluate confidence\n",
      "        confidence_scores = {agent: self._evaluate_confidence(responses[agent]) for agent in responses}\n",
      "        weights = self._adjust_weights(prompt, confidence_scores)\n",
      "\n",
      "        # Aggregate responses\n",
      "        final_response = self._aggregate_responses(responses, weights)\n",
      "\n",
      "        # Store memory\n",
      "        for agent, score in confidence_scores.items():\n",
      "            self.voting_memory[prompt].append(score)\n",
      "\n",
      "        return {\n",
      "            'content': final_response,\n",
      "            'extra': {\n",
      "                'raw_responses': responses,\n",
      "                'confidence_scores': confidence_scores,\n",
      "                'weights': weights\n",
      "            }\n",
      "        }\n"
     ]
    }
   ],
   "source": [
    "# best discovered block\n",
    "best_score = 0\n",
    "best_block = None\n",
    "for block in archive:\n",
    "    if block[\"fitness\"][\"median\"] > best_score:\n",
    "        best_score = block[\"fitness\"][\"median\"]\n",
    "        best_block = block\n",
    "\n",
    "print(best_block[\"name\"], \"- Score: \", best_block[\"fitness\"][\"median\"], end=\"\\n\\n\")\n",
    "print(best_block[\"thought\"], end=\"\\n\\n\")\n",
    "print(best_block[\"code\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15ce207",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot block discovery progress\n",
    "\n",
    "medians = [float(item['fitness']['median']) for item in archive]\n",
    "is_builtin = [item['thought'] == 'agenticblocks built-in' for item in archive]\n",
    "\n",
    "builtin_items = [(item['name'], float(item['fitness']['median'])) for item in archive if item['thought'] == 'agenticblocks built-in']\n",
    "discovered_items = [(item['name'], float(item['fitness']['median'])) for item in archive if item['thought'] != 'agenticblocks built-in']\n",
    "\n",
    "best_so_far = []\n",
    "best_names = []\n",
    "current_best = 0\n",
    "current_best_name = ''\n",
    "for name, score in discovered_items:\n",
    "    if score > current_best:\n",
    "        current_best = score\n",
    "        current_best_name = name\n",
    "    best_so_far.append(current_best)\n",
    "    best_names.append(current_best_name if score >= current_best else None)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "colors = plt.cm.tab10.colors\n",
    "for i, (name, score) in enumerate(builtin_items):\n",
    "    plt.axhline(y=score, linestyle='--', alpha=0.7, color=colors[i], label=name)\n",
    "\n",
    "iterations = range(1, len(best_so_far) + 1)\n",
    "plt.plot(iterations, best_so_far, 'ko-', linewidth=2, markersize=8, label='Best discovered')\n",
    "for i, name in enumerate(best_names):\n",
    "    if name and (i == 0 or best_so_far[i] > best_so_far[i-1]):\n",
    "        plt.annotate(name, (i+1, best_so_far[i]), textcoords=\"offset points\", \n",
    "                     xytext=(5, 5), fontsize=8)\n",
    "\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('ADAS Block Discovery Progress')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cc59fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
