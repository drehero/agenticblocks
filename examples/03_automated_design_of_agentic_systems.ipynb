{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d2fe7a1",
   "metadata": {},
   "source": [
    "# Automated Design of Agentic Systems (ADAS)\n",
    "\n",
    "Automated Design of Agentic Systems (ADAS) is a [paper by Hu et al.](https://arxiv.org/pdf/2408.08435) which was one of the first works to explore the idea of automatically designing agentic systems:\n",
    "\n",
    ">Automated Design of Agentic Systems (ADAS) [...] aims to automatically create powerful agentic system designs, including inventing novel building blocks\n",
    "and/or combining them in new ways.\n",
    "\n",
    "The idea of ADAS is that instead of using hand-crafted prompting techniques like LLM debate, a meta agent designs novel agents using code. This agent is then tested on a task and the results are given back to the meta agent to further guide its search for a design.\n",
    "\n",
    "![](assets/adas.png)\n",
    "\n",
    "```\n",
    "@article{hu2024ADAS,\n",
    "title={Automated Design of Agentic Systems},\n",
    "author={Hu, Shengran and Lu, Cong and Clune, Jeff},\n",
    "journal={arXiv preprint arXiv:2408.08435},\n",
    "year={2024}\n",
    "}\n",
    "```\n",
    "\n",
    "This notebook shows how to implement their ideas using the agenticblocks framework."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b566d4",
   "metadata": {},
   "source": [
    "To implement ADAS we need to build the following:\n",
    "\n",
    "1. The meta agent which has the task to design agentic systems. We give it access to the built-in blocks of agenticblocks and ask it to come up with novel blocks and / or combine them in new ways.\n",
    "2. A way of testing the designs found by the meta agent, so it can use the feedback as input for new designs. For demonstration purposes we will use a small sample of the [MMLU-STEM Dataset](https://huggingface.co/datasets/TIGER-Lab/MMLU-STEM), containing multiple choice questions from academic exams and textbooks in the STEM domain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ce00a6",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f9121d",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "Besides some built-in python libraries, we import the agenticblocks library to define the agentic system, pyarrow to read the MMLU dataset, numpy to compute metrics and tqdm to show progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab848f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/robin/projects/agenticblocks/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "import io\n",
    "import json\n",
    "import urllib.request\n",
    "import traceback\n",
    "\n",
    "import agenticblocks as ab\n",
    "import numpy as np\n",
    "import pyarrow.parquet as pq\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3048b680",
   "metadata": {},
   "source": [
    "This example involves running model-generated code. If you set `YOLO = False` you will be prompted to review and accept any model-generated code before it is executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60770bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "YOLO = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e4ff53",
   "metadata": {},
   "source": [
    "### Model Access\n",
    "\n",
    "We need to set up access to the language model(s) we want to use.\n",
    "agenticblocks supports all OpenAI API compatible providers.\n",
    "\n",
    "You can set the base url and api key via the `OPENAI_API_URL` and `OPENAI_API_KEY` environment variables.\n",
    "\n",
    "For more details check the [getting started example](01_getting_started.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38bca8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import dotenv\n",
    "#dotenv.load_dotenv()\n",
    "\n",
    "#!export OPENAI_API_URL=\n",
    "#!export OPENAI_API_KEY="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52178805",
   "metadata": {},
   "source": [
    "We also define the models we are going to use throughout this notebook.\n",
    "For this example we will use gpt-4o for the meta agent and gpt-3.5-turbo as the agent in the discovered blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d68f528",
   "metadata": {},
   "outputs": [],
   "source": [
    "META_MODEL_NAME = \"openai/gpt-4o\"\n",
    "BLOCK_MODEL_NAME = \"openai/gpt-3.5-turbo\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622e9ea0",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "Let's download the MMLU STEM dataset from Hugging Face.\n",
    "\n",
    "We will use a small subset of 50 examples for providing feedback to the meta agent on how well the discovered block perfomed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "553c16a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 50\n",
    "url = \"https://huggingface.co/datasets/TIGER-Lab/MMLU-STEM/resolve/main/data/test-00000-of-00001.parquet\"\n",
    "\n",
    "with urllib.request.urlopen(url) as response:\n",
    "    data = io.BytesIO(response.read())\n",
    "\n",
    "df = pq.read_table(data).to_pandas().sample(n_samples, random_state=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0c66bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_input(row):\n",
    "    return f\"\"\"{row['question']}\n",
    "\n",
    "{chr(10).join([f\"{chr(ord('A')+i)}) {choice}\" for i, choice in enumerate(row['choices'])])}\n",
    "\"\"\"\n",
    "\n",
    "df[\"input\"] = df.apply(format_input, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d3f02a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statement 1 | For any two groups G and G', there exists a homomorphism of G into G'. Statement 2 | Every homomorphism is a one-to-one map.\n",
      "\n",
      "A) True, True\n",
      "B) False, False\n",
      "C) True, False\n",
      "D) False, True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df[\"input\"].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a0a122",
   "metadata": {},
   "source": [
    "## The Meta Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42aa3a28",
   "metadata": {},
   "source": [
    "The meta agent itself is just an instance of the agenticblocks `Model` class. The same class the agent will be use to create its models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0934fa69",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_agent = ab.Model(\n",
    "    META_MODEL_NAME,\n",
    "    keep_history=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16ec684",
   "metadata": {},
   "source": [
    "We also need to define the prompt for the meta agent. In the prompt we let the meta agent know ...\n",
    "* about its task - building an agentic system to answer MMLU questions\n",
    "* what tools it can use to build these systems - python code and the agenticblocks framework\n",
    "* how its blocks will be evaluated on the dataset\n",
    "* how its previous solutions performed by using an \"archive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57edfc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks = [ab.IO, ab.ChainOfThought, ab.SelfConsistency, ab.MultiAgentDebate, ab.SelfRefine]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce5faf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"# Overview\n",
    "You are an expert machine learning researcher testing various agentic systems. Your objective is to design building blocks such as prompts and control flows within these systems to solve complex tasks. Your aim is to design an optimal agent performing well on the MMLU (Massive Multitask Language Understanding) benchmark, a challenging evaluation that assesses a model's ability to answer questions across a wide range of subjects and difficulty levels. It includes subjects from STEM, social sciences, humanities, and more.\n",
    "\n",
    "## An example question from MMLU:\n",
    "The constellation ... is a bright W-shaped constellation in the northern sky.\n",
    "\n",
    "A) Centaurus\n",
    "B) Cygnus\n",
    "C) Cassiopeia\n",
    "D) Cepheus\n",
    "\n",
    "# How to implement your agentic systems\n",
    "You can use the agenticblocks library to create and manage models:\n",
    "\n",
    "```python\n",
    "import agenticblocks as ab\n",
    "model = ab.Model(\"{BLOCK_MODEL_NAME}\", system_prompt=\"You are a helpful assistant.\")\n",
    "```\n",
    "\n",
    "Always use the {BLOCK_MODEL_NAME} model when you create a new model.\n",
    "A model can be prompted in the following way:\n",
    "\n",
    "```python\n",
    "model(\"Your prompt here\", temperature=0.7)\n",
    "```\n",
    "\n",
    "blocks define how models are prompted and how they interact with each other.\n",
    "agenticblocks offers some built-in blocks that you can use to create agentic systems:\n",
    "\n",
    "```python\n",
    "{'\\n\\n\\n'.join([inspect.getsource(block) for block in blocks])}\n",
    "```\n",
    "\n",
    "The built-in blocks can be imported using `from agenticblocks import BlockName`.\n",
    "You can use these blocks as building blocks to combine them to new blocks or use them as inspiration to design your own blocks.\n",
    "\n",
    "# Discovered architecture archive\n",
    "Here is the archive of the built-in blocks and discovered architectures:\n",
    "\n",
    "[ARCHIVE]\n",
    "\n",
    "The fitness value is the median and 95% Bootstrap Confidence Interval of the correct rate on a validation question set. Your GOAL is to maximize the \"fitness\".\n",
    "\n",
    "# Output Instruction and Example:\n",
    "You need to output a JSON object with three keys: \"thought\", \"name\", and \"code\".\n",
    "The first key should be (\"thought\"), and it should capture your thought process for designing the next function. In the \"thought\" section, first reason about what should be the next interesting agent to try, then describe your reasoning and the overall concept behind the agent design, and finally detail the implementation steps.\n",
    "The second key (\"name\") corresponds to the class name of your next agent architecture. Make sure the name is unique and descriptive of the architecture you are proposing. \n",
    "Finally, the last key (\"code\") corresponds to the exact Python code of the class that you would like to try. You must write a COMPLETE CODE in \"code\": Your code will be part of the entire project, so please implement complete, reliable, reusable code snippets.\n",
    "\n",
    "Here is an example of the output format for the next agent architecture:\n",
    "\n",
    "{{\n",
    "    \"thought\": \"**Insights:**\\nYour insights on what should be the next interesting agent.\\n**Overall Idea:**\\nyour reasoning and the overall concept behind the agent design.\\n**Implementation:**\\ndescribe the implementation step by step.\",\n",
    "    \"name\": \"YourCustomAgenticBlock\",\n",
    "    \"code\": '''class YourCustomAgenticBlock:\n",
    "    def __init__(self):\n",
    "        # Your code here\n",
    "\n",
    "    def __call__(self, question):\n",
    "        # Your code here\n",
    "'''\n",
    "}}\n",
    "\n",
    "You must use the exact interface used above. You need to specify the instruction, input information, and the required output fields for various LLM agents to do their specific part of the architecture. \n",
    "Also, it could be helpful to set the LLM’s system_prompt and temperature to further control the LLM’s response. Note that only the question will be passed to the instance of the block class. Everything else needs to be already implemented in the __init__ and __call__ methods of the block class.\n",
    "DO NOT FORGET to pass the task description to models inside the block class if you think it is needed, otherwise the models will not know about the task.\n",
    "\n",
    "The key \"code\" from your output JSON will be saved to a custom_blocks.py file and will be called like this:\n",
    "\n",
    "```python\n",
    "from custom_blocks import YourCustomAgenticBlock\n",
    "\n",
    "question = '''The constellation ... is a bright W-shaped constellation in the northern sky.\n",
    "\n",
    "A) Centaurus\n",
    "B) Cygnus\n",
    "C) Cassiopeia\n",
    "D) Cepheus\n",
    "'''\n",
    "\n",
    "block = YourCustomAgenticBlock()\n",
    "result = block(question)\n",
    "```\n",
    "\n",
    "# Your task\n",
    "You are deeply familiar with LLM prompting techniques and LLM agent works from the literature. Your goal is to maximize \"fitness\" by proposing interestingly new agents. \n",
    "Observe the discovered architectures carefully and think about what insights, lessons, or stepping stones can be learned from them.\n",
    "Be creative to think about the next interesting architecture to try. You are encouraged to draw inspiration from related LLM agent papers or academic papers from other research areas.\n",
    "Using the knowledge learned from the archive and the inspiration from academic literature to give the next interesting architecture.\n",
    "THINK OUTSIDE THE BOX.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a5eed8",
   "metadata": {},
   "source": [
    "We will prompt the meta agent in a self refining manner. That means we ask it one time to reflect on its design and improve it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed44a9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "reflection_template = \"\"\"\"{previous_response}Carefully review the proposed new architecture and reflect on the following points:\"\n",
    "\n",
    "1. **Interestingness**: Assess whether your proposed architecture is interesting or innovative compared to existing methods in the archive. If you determine that the proposed architecture is not interesting, suggest a new architecture that addresses these shortcomings. \n",
    "- Make sure to check the difference between the proposed architecture and previous attempts.\n",
    "- Compare the proposal and the architectures in the archive CAREFULLY, including their actual differences in the implementation.\n",
    "- Decide whether the current architecture is innovative.\n",
    "- USE CRITICAL THINKING!\n",
    "\n",
    "2. **Implementation Mistakes**: Identify any mistakes you may have made in the implementation. Review the code carefully, debug any issues you find, and provide a corrected version. REMEMBER checking \"## WRONG Implementation examples\" in the prompt.\n",
    "\n",
    "3. **Improvement**: Based on the proposed architecture, suggest improvements in the detailed implementation that could increase its performance or effectiveness. In this step, focus on refining and optimizing the existing implementation without altering the overall design framework, except if you want to propose a different architecture if the current is not interesting.\n",
    "- Observe carefully about whether the implementation is actually doing what it is supposed to do.\n",
    "- Check if there is redundant code or unnecessary steps in the implementation. Replace them with effective implementation.\n",
    "- Try to avoid the implementation being too similar to the previous agent.\n",
    "\n",
    "And then, you need to improve or revise the implementation, or implement the new proposed architecture based on the reflection.\n",
    "\n",
    "Your response should be organized as follows:\n",
    "\n",
    "\"reflection\": Provide your thoughts on the interestingness of the architecture, identify any mistakes in the implementation, and suggest improvements.\n",
    "\n",
    "\"thought\": Revise your previous proposal or propose a new architecture if necessary, using the same format as the example response.\n",
    "\n",
    "\"name\": Provide a name for the revised or new architecture. (Don't put words like \"new\" or \"improved\" in the name.)\n",
    "\n",
    "\"code\": Provide the corrected code or an improved implementation. Make sure you actually implement your fix and improvement in this code.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd6141e",
   "metadata": {},
   "source": [
    "This is an adjusted prompt from the [ADAS repo](https://github.com/ShengranHu/ADAS).\n",
    "\n",
    "For simplicity, we tell the meta agent to always use the `MODEL_NAME` defined above for its blocks.\n",
    "If you have more models available, you can modify the prompt to let the meta agent decide which models it wants to use.\n",
    "\n",
    "We also show the meta agent the built in blocks and how it can design its own blocks.\n",
    "\n",
    "The archive is initialized by evaluating the built in blocks on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb5c1553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to evaluate a block on the mmlu dataset\n",
    "def eval_block(block, df):\n",
    "    answers = []\n",
    "    for question in df[\"input\"]:\n",
    "        response = block(f\"{question}\\n\\nThe last line of your answer should be the correct choice, e.g. A)\")[\"content\"]\n",
    "        answer = None\n",
    "        for line in response.splitlines():\n",
    "            if answer is not None:\n",
    "                break\n",
    "            for k, v in {\"A)\": 0, \"B)\": 1, \"C)\": 2, \"D)\": 3}.items():\n",
    "                if k in line:\n",
    "                    answer = v\n",
    "                    break\n",
    "        answers += [answer]\n",
    "    df[str(block)] = answers\n",
    "    # bootstrap score\n",
    "    acc_list = (df[\"answer\"] == df[str(block)]).astype(int)\n",
    "    scores = []\n",
    "    for _ in range(1000):\n",
    "        scores += [np.mean(np.random.choice(acc_list, len(acc_list), replace=True))]\n",
    "    return np.percentile(scores, 2.5), np.percentile(scores, 97.5), np.median(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad9bb12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# build the initial archive by evaluating the built-in blocks on the train set\n",
    "archive = []\n",
    "for block_class in tqdm(blocks, total=len(blocks)):\n",
    "    model = ab.Model(BLOCK_MODEL_NAME)\n",
    "    if block_class == ab.MultiAgentDebate:\n",
    "        block = block_class(agents=[model]*3)\n",
    "    else:\n",
    "        block = block_class(model)\n",
    "    ci_lo, ci_hi, med = eval_block(block, df)\n",
    "    archive += [{\n",
    "        \"name\": f\"ab.{block_class.__name__}\",\n",
    "        \"thought\": \"agenticblocks built-in\",\n",
    "        \"code\": inspect.getsource(block_class),\n",
    "        \"fitness\": {\"95% Bootstrap Confidence Interval\": f\"({ci_lo*100:.1f}%, {ci_hi*100:.1f}%)\", \"median\": round(med*100, 1)},\n",
    "    }]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0a44f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'ab.IO',\n",
       "  'thought': 'agenticblocks built-in',\n",
       "  'code': 'class IO:\\n    \"\"\"IO block - simple pass-through to the model.\"\"\"\\n\\n    def __init__(self, model):\\n        self.model = model\\n\\n    def __repr__(self):\\n        return f\"IO({self.model!r})\"\\n\\n    def __call__(self, prompt: str, **kwargs) -> dict:\\n        return self.model(prompt, **kwargs)\\n',\n",
       "  'fitness': {'95% Bootstrap Confidence Interval': '(42.0%, 68.0%)',\n",
       "   'median': np.float64(56.0)}},\n",
       " {'name': 'ab.ChainOfThought',\n",
       "  'thought': 'agenticblocks built-in',\n",
       "  'code': 'class ChainOfThought:\\n    \"\"\"Chain of Thought block - prompts the model to think step by step.\"\"\"\\n\\n    def __init__(self, model, template: str = \"{prompt}\\\\nLet\\'s think step by step.\"):\\n        self.model = model\\n        self.template = template\\n\\n    def __repr__(self):\\n        return f\"ChainOfThought({self.model!r})\"\\n\\n    def __call__(self, prompt: str, **kwargs) -> dict:\\n        return self.model(self.template.format(prompt=prompt), **kwargs)\\n',\n",
       "  'fitness': {'95% Bootstrap Confidence Interval': '(38.0%, 66.0%)',\n",
       "   'median': np.float64(52.0)}},\n",
       " {'name': 'ab.SelfConsistency',\n",
       "  'thought': 'agenticblocks built-in',\n",
       "  'code': 'class SelfConsistency:\\n    \"\"\"Self-Consistency block - runs a block N times and aggregates responses.\"\"\"\\n\\n    def __init__(\\n        self,\\n        block,\\n        n: int = 5,\\n        temperature: float = 0.7,\\n        aggregator=None,\\n        aggregator_template: str = \"{responses}\\\\nGiven the responses above. Output the most common answer.\",\\n    ):\\n        self.block = block\\n        self.n = n\\n        self.temperature = temperature\\n        self.aggregator = aggregator\\n        self.aggregator_template = aggregator_template\\n\\n    def __repr__(self):\\n        return f\"SelfConsistency({self.block!r}, n={self.n})\"\\n\\n    def __call__(self, prompt: str, **kwargs) -> dict:\\n        responses = []\\n        for _ in range(self.n):\\n            result = self.block(prompt, temperature=self.temperature, **kwargs)\\n            responses.append(result[\"content\"])\\n\\n        responses_text = \"\\\\n\\\\n\".join(responses)\\n\\n        if self.aggregator is None:\\n            return {\"content\": responses_text, \"extra\": {\"responses\": responses}}\\n\\n        return self.aggregator(self.aggregator_template.format(responses=responses_text))\\n',\n",
       "  'fitness': {'95% Bootstrap Confidence Interval': '(44.0%, 72.0%)',\n",
       "   'median': np.float64(58.0)}},\n",
       " {'name': 'ab.MultiAgentDebate',\n",
       "  'thought': 'agenticblocks built-in',\n",
       "  'code': 'class MultiAgentDebate:\\n    \"\"\"Multi-Agent Debate block - multiple agents debate to reach a consensus.\"\"\"\\n\\n    def __init__(\\n        self,\\n        agents: list,\\n        rounds: int = 2,\\n        moderator=None,\\n        debate_template: str = \"Question: {prompt}\\\\n\\\\nPrevious responses:\\\\n{history}\\\\n\\\\nProvide your response, considering the perspectives above:\",\\n        final_template: str = \"Question: {prompt}\\\\n\\\\nDebate summary:\\\\n{debate_history}\\\\n\\\\nBased on this debate, provide the final answer:\",\\n    ):\\n        self.agents = agents\\n        self.rounds = rounds\\n        self.moderator = moderator\\n        self.debate_template = debate_template\\n        self.final_template = final_template\\n\\n    def __repr__(self):\\n        return f\"MultiAgentDebate({self.agents!r}, rounds={self.rounds})\"\\n\\n    def __call__(self, prompt: str, **kwargs) -> dict:\\n        debate_history = []\\n\\n        # Initial round - each agent responds to the prompt\\n        for i, agent in enumerate(self.agents):\\n            result = agent(prompt, **kwargs)\\n            debate_history.append(f\"Agent {i + 1}: {result[\\'content\\']}\")\\n\\n        # Debate rounds\\n        for _ in range(self.rounds):\\n            history_text = \"\\\\n\\\\n\".join(debate_history)\\n            round_responses = []\\n\\n            for i, agent in enumerate(self.agents):\\n                result = agent(\\n                    self.debate_template.format(prompt=prompt, history=history_text),\\n                    **kwargs,\\n                )\\n                round_responses.append(f\"Agent {i + 1}: {result[\\'content\\']}\")\\n\\n            debate_history.extend(round_responses)\\n\\n        # Final synthesis\\n        full_history = \"\\\\n\\\\n\".join(debate_history)\\n\\n        if self.moderator is not None:\\n            final_result = self.moderator(\\n                self.final_template.format(prompt=prompt, debate_history=full_history),\\n                **kwargs,\\n            )\\n        else:\\n            # Use last agent as moderator if none provided\\n            final_result = self.agents[-1](\\n                self.final_template.format(prompt=prompt, debate_history=full_history),\\n                **kwargs,\\n            )\\n\\n        return {\\n            \"content\": final_result[\"content\"],\\n            \"extra\": {\"debate_history\": debate_history},\\n        }\\n',\n",
       "  'fitness': {'95% Bootstrap Confidence Interval': '(44.0%, 72.0%)',\n",
       "   'median': np.float64(58.0)}},\n",
       " {'name': 'ab.SelfRefine',\n",
       "  'thought': 'agenticblocks built-in',\n",
       "  'code': 'class SelfRefine:\\n    \"\"\"Self-Refine block - iteratively critiques and improves responses.\"\"\"\\n\\n    def __init__(\\n        self,\\n        model,\\n        iterations: int = 2,\\n        critique_template: str = \"Task: {prompt}\\\\n\\\\nResponse:\\\\n{response}\\\\n\\\\nCritique this response. What are its weaknesses? How can it be improved?\",\\n        refine_template: str = \"Task: {prompt}\\\\n\\\\nPrevious response:\\\\n{response}\\\\n\\\\nCritique:\\\\n{critique}\\\\n\\\\nProvide an improved response addressing the critique:\",\\n    ):\\n        self.model = model\\n        self.iterations = iterations\\n        self.critique_template = critique_template\\n        self.refine_template = refine_template\\n\\n    def __repr__(self):\\n        return f\"SelfRefine({self.model!r}, iterations={self.iterations})\"\\n\\n    def __call__(self, prompt: str, **kwargs) -> dict:\\n        # Generate initial response\\n        result = self.model(prompt, **kwargs)\\n        response = result[\"content\"]\\n\\n        history = [{\"response\": response, \"critique\": None}]\\n\\n        # Iterative refinement\\n        for _ in range(self.iterations):\\n            # Critique\\n            critique_result = self.model(\\n                self.critique_template.format(prompt=prompt, response=response),\\n                **kwargs,\\n            )\\n            critique = critique_result[\"content\"]\\n\\n            # Refine\\n            refine_result = self.model(\\n                self.refine_template.format(prompt=prompt, response=response, critique=critique),\\n                **kwargs,\\n            )\\n            response = refine_result[\"content\"]\\n\\n            history.append({\"response\": response, \"critique\": critique})\\n\\n        return {\\n            \"content\": response,\\n            \"extra\": {\"history\": history},\\n        }\\n',\n",
       "  'fitness': {'95% Bootstrap Confidence Interval': '(44.0%, 70.0%)',\n",
       "   'median': np.float64(58.0)}}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "archive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06c8d92",
   "metadata": {},
   "source": [
    "Now let's use the prompts to let the meta agent search for new blocks, evaluate them on the mmlu dataset and append new blocks to the archive.\n",
    "\n",
    "For demonstration purposes we do this for 15 rounds. For better results add more rounds - in the ADAS paper 25 iterations are used.\n",
    "\n",
    "If the code produced by the meta agent throws an exception we ask it up to 5 times to fix it.\n",
    "\n",
    "If you notice the meta agent keeps repeating a certain mistake or uses a bad pattern in the generated blocks you can adjust the prompt above and ask it to avoid these mistakes.\n",
    "\n",
    "With `YOLO = False`, you need to confirm each new block before it is evaluated on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46961744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================== Round 0 of 15 ==============================\n",
      "\n",
      "RefinedDomainExpertCollaboration:\n",
      "class RefinedDomainExpertCollaboration:\n",
      "    def __init__(self, domains):\n",
      "        from agenticblocks import SelfRefine\n",
      "        # Initialize experts for each domain, allowing refinement iterations\n",
      "        self.models = [\n",
      "            SelfRefine(\n",
      "                ab.Model(\"openai/gpt-3.5-turbo\", system_prompt=f\"You are an expert in {domain}.\")\n",
      "            , iterations=2\n",
      "            ) for domain in domains\n",
      "        ]\n",
      "        # Synthesizer model for final synthesis\n",
      "        self.synthesizer_model = ab.Model(\n",
      "            \"openai/gpt-3.5-turbo\",\n",
      "            system_prompt=\"You are tasked with synthesizing and weighting expert insights into the most accurate knowledge.\"\n",
      "        )\n",
      "\n",
      "    def __call__(self, question: str, **kwargs) -> dict:\n",
      "        responses = []\n",
      "        # Variable temperature might yield diverse outcomes\n",
      "        temperatures = [0.6, 0.7, 0.8]\n",
      "        for i, model in enumerate(self.models):\n",
      "            response = model(question=question, temperature=temperatures[i % len(temperatures)])\n",
      "            responses.append(f\"Domain Expert {i + 1}: {response['content']}\")\n",
      "        responses_text = \"\\n\\n\".join(responses)\n",
      "        # Weighted finalization of outputs\n",
      "        final_response = self.synthesizer_model(\n",
      "            f\"Question: {question}\\n\\nWeighted expert insights:\\n{responses_text}\\n\\nProvide the best synthesized answer, considering relevance and confidence.\",\n",
      "            temperature=0.7\n",
      "        )\n",
      "        return {\n",
      "            \"content\": final_response[\"content\"],\n",
      "            \"extra\": {\"responses\": responses}\n",
      "        }\n",
      "\n",
      "# Example usage\n",
      "# domains = [\"STEM\", \"humanities\", \"social sciences\"]\n",
      "# block = RefinedDomainExpertCollaboration(domains)\n",
      "# question = 'The constellation ... is a bright W-shaped constellation in the northern sky. A) Centaurus B) Cygnus C) Cassiopeia D) Cepheus'\n",
      "# result = block(question)\n",
      "# print(result['content'])\n",
      "\n",
      "Error (attempt 1/3): RefinedDomainExpertCollaboration.__init__() missing 1 required positional argument: 'domains'\n"
     ]
    }
   ],
   "source": [
    "n_iters = 15\n",
    "for iter in range(n_iters):\n",
    "    print(\"=\"*30, f\"Round {iter} of {n_iters}\", \"=\"*30)\n",
    "    meta_agent.reset_history()  # reset the models message history\n",
    "    response = meta_agent(prompt.replace(\"[ARCHIVE]\", json.dumps(archive)))\n",
    "    response = meta_agent(reflection_template.format(previous_response=json.dumps(response)))\n",
    "\n",
    "    success = False\n",
    "    for retry in range(5):\n",
    "        try:\n",
    "            # Parse JSON response\n",
    "            response_parsed = json.loads(response[\"content\"].split('```json\\n')[1].split('\\n```')[0])\n",
    "            \n",
    "            # Review mode: show code and ask for approval\n",
    "            if not YOLO:\n",
    "                print(f\"\\n{response_parsed['name']}:\\n{response_parsed['code']}\\n\")\n",
    "                if input(\"Run? [Y/n]: \").strip().lower() == 'n':\n",
    "                    break\n",
    "            \n",
    "            # Execute the code\n",
    "            namespace = globals().copy()\n",
    "            exec(response_parsed[\"code\"], namespace)\n",
    "            NewBlock = namespace[response_parsed[\"name\"]]\n",
    "            block = NewBlock()\n",
    "            ci_lo, ci_hi, med = eval_block(block, df)\n",
    "            success = True\n",
    "            break\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error (attempt {retry + 1}/3): {e}\")\n",
    "            response = meta_agent(f\"The following exception occurred when trying to test the block: {e}. Traceback: {traceback.format_exc()}. Return a valid JSON with keys name, code, thought and reflection. The name is the class name of the found block, code contains the fixed implementation of the block, thought as before the thought process behind the block design and reflection your reflection on the exception.\")\n",
    "\n",
    "    if success:\n",
    "        result = {\n",
    "            \"name\": NewBlock.__name__,\n",
    "            \"thought\": response_parsed[\"thought\"],\n",
    "            \"code\": response_parsed[\"code\"],\n",
    "            \"fitness\": {\"95% Bootstrap Confidence Interval\": f\"({ci_lo*100:.1f}%, {ci_hi*100:.1f}%)\", \"median\": round(med*100, 1)},\n",
    "        }\n",
    "        archive += [result]\n",
    "        print(\"Successfully found a new block:\")\n",
    "        print(json.dumps(result, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf17c5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'ab.IO',\n",
       "  'thought': 'agenticblocks built-in',\n",
       "  'code': 'class IO:\\n    \"\"\"IO block - simple pass-through to the model.\"\"\"\\n\\n    def __init__(self, model):\\n        self.model = model\\n\\n    def __repr__(self):\\n        return f\"IO({self.model!r})\"\\n\\n    def __call__(self, prompt: str, **kwargs) -> dict:\\n        return self.model(prompt, **kwargs)\\n',\n",
       "  'fitness': '95% Bootstrap Confidence Interval: (40.0%, 68.0%), Median: 54.0%'},\n",
       " {'name': 'ab.ChainOfThought',\n",
       "  'thought': 'agenticblocks built-in',\n",
       "  'code': 'class ChainOfThought:\\n    \"\"\"Chain of Thought block - prompts the model to think step by step.\"\"\"\\n\\n    def __init__(self, model, template: str = \"{prompt}\\\\nLet\\'s think step by step.\"):\\n        self.model = model\\n        self.template = template\\n\\n    def __repr__(self):\\n        return f\"ChainOfThought({self.model!r})\"\\n\\n    def __call__(self, prompt: str, **kwargs) -> dict:\\n        return self.model(self.template.format(prompt=prompt), **kwargs)\\n',\n",
       "  'fitness': '95% Bootstrap Confidence Interval: (32.0%, 60.0%), Median: 46.0%'},\n",
       " {'name': 'ab.SelfConsistency',\n",
       "  'thought': 'agenticblocks built-in',\n",
       "  'code': 'class SelfConsistency:\\n    \"\"\"Self-Consistency block - runs a block N times and aggregates responses.\"\"\"\\n\\n    def __init__(\\n        self,\\n        block,\\n        n: int = 5,\\n        temperature: float = 0.7,\\n        aggregator=None,\\n        aggregator_template: str = \"{responses}\\\\nGiven the responses above. Output the most common answer.\",\\n    ):\\n        self.block = block\\n        self.n = n\\n        self.temperature = temperature\\n        self.aggregator = aggregator\\n        self.aggregator_template = aggregator_template\\n\\n    def __repr__(self):\\n        return f\"SelfConsistency({self.block!r}, n={self.n})\"\\n\\n    def __call__(self, prompt: str, **kwargs) -> dict:\\n        responses = []\\n        for _ in range(self.n):\\n            result = self.block(prompt, temperature=self.temperature, **kwargs)\\n            responses.append(result[\"content\"])\\n\\n        responses_text = \"\\\\n\\\\n\".join(responses)\\n\\n        if self.aggregator is None:\\n            return {\"content\": responses_text, \"extra\": {\"responses\": responses}}\\n\\n        return self.aggregator(self.aggregator_template.format(responses=responses_text))\\n',\n",
       "  'fitness': '95% Bootstrap Confidence Interval: (48.0%, 76.0%), Median: 62.0%'},\n",
       " {'name': 'ab.MultiAgentDebate',\n",
       "  'thought': 'agenticblocks built-in',\n",
       "  'code': 'class MultiAgentDebate:\\n    \"\"\"Multi-Agent Debate block - multiple agents debate to reach a consensus.\"\"\"\\n\\n    def __init__(\\n        self,\\n        agents: list,\\n        rounds: int = 2,\\n        moderator=None,\\n        debate_template: str = \"Question: {prompt}\\\\n\\\\nPrevious responses:\\\\n{history}\\\\n\\\\nProvide your response, considering the perspectives above:\",\\n        final_template: str = \"Question: {prompt}\\\\n\\\\nDebate summary:\\\\n{debate_history}\\\\n\\\\nBased on this debate, provide the final answer:\",\\n    ):\\n        self.agents = agents\\n        self.rounds = rounds\\n        self.moderator = moderator\\n        self.debate_template = debate_template\\n        self.final_template = final_template\\n\\n    def __repr__(self):\\n        return f\"MultiAgentDebate({self.agents!r}, rounds={self.rounds})\"\\n\\n    def __call__(self, prompt: str, **kwargs) -> dict:\\n        debate_history = []\\n\\n        # Initial round - each agent responds to the prompt\\n        for i, agent in enumerate(self.agents):\\n            result = agent(prompt, **kwargs)\\n            debate_history.append(f\"Agent {i + 1}: {result[\\'content\\']}\")\\n\\n        # Debate rounds\\n        for _ in range(self.rounds):\\n            history_text = \"\\\\n\\\\n\".join(debate_history)\\n            round_responses = []\\n\\n            for i, agent in enumerate(self.agents):\\n                result = agent(\\n                    self.debate_template.format(prompt=prompt, history=history_text),\\n                    **kwargs,\\n                )\\n                round_responses.append(f\"Agent {i + 1}: {result[\\'content\\']}\")\\n\\n            debate_history.extend(round_responses)\\n\\n        # Final synthesis\\n        full_history = \"\\\\n\\\\n\".join(debate_history)\\n\\n        if self.moderator is not None:\\n            final_result = self.moderator(\\n                self.final_template.format(prompt=prompt, debate_history=full_history),\\n                **kwargs,\\n            )\\n        else:\\n            # Use last agent as moderator if none provided\\n            final_result = self.agents[-1](\\n                self.final_template.format(prompt=prompt, debate_history=full_history),\\n                **kwargs,\\n            )\\n\\n        return {\\n            \"content\": final_result[\"content\"],\\n            \"extra\": {\"debate_history\": debate_history},\\n        }\\n',\n",
       "  'fitness': '95% Bootstrap Confidence Interval: (44.0%, 70.0%), Median: 58.0%'},\n",
       " {'name': 'ab.SelfRefine',\n",
       "  'thought': 'agenticblocks built-in',\n",
       "  'code': 'class SelfRefine:\\n    \"\"\"Self-Refine block - iteratively critiques and improves responses.\"\"\"\\n\\n    def __init__(\\n        self,\\n        model,\\n        iterations: int = 2,\\n        critique_template: str = \"Task: {prompt}\\\\n\\\\nResponse:\\\\n{response}\\\\n\\\\nCritique this response. What are its weaknesses? How can it be improved?\",\\n        refine_template: str = \"Task: {prompt}\\\\n\\\\nPrevious response:\\\\n{response}\\\\n\\\\nCritique:\\\\n{critique}\\\\n\\\\nProvide an improved response addressing the critique:\",\\n    ):\\n        self.model = model\\n        self.iterations = iterations\\n        self.critique_template = critique_template\\n        self.refine_template = refine_template\\n\\n    def __repr__(self):\\n        return f\"SelfRefine({self.model!r}, iterations={self.iterations})\"\\n\\n    def __call__(self, prompt: str, **kwargs) -> dict:\\n        # Generate initial response\\n        result = self.model(prompt, **kwargs)\\n        response = result[\"content\"]\\n\\n        history = [{\"response\": response, \"critique\": None}]\\n\\n        # Iterative refinement\\n        for _ in range(self.iterations):\\n            # Critique\\n            critique_result = self.model(\\n                self.critique_template.format(prompt=prompt, response=response),\\n                **kwargs,\\n            )\\n            critique = critique_result[\"content\"]\\n\\n            # Refine\\n            refine_result = self.model(\\n                self.refine_template.format(prompt=prompt, response=response, critique=critique),\\n                **kwargs,\\n            )\\n            response = refine_result[\"content\"]\\n\\n            history.append({\"response\": response, \"critique\": critique})\\n\\n        return {\\n            \"content\": response,\\n            \"extra\": {\"history\": history},\\n        }\\n',\n",
       "  'fitness': '95% Bootstrap Confidence Interval: (40.0%, 68.0%), Median: 54.0%'}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710d2f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO plot fitness based on archive"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
